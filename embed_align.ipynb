{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import spatial\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions.kl as kl\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "from random import randint\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('error')\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_count = 5\n",
    "embedding_dim = 100\n",
    "det_embedding_dim = 128\n",
    "batch_size = 100\n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "window_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read a subset 10000 sentences\n",
    "with open('corpus2id_hansard_en_subs.pickle', 'rb') as f:\n",
    "    corpus2id_en = pickle.load(f)\n",
    "    \n",
    "with open('corpus2id_hansard_fr_subs.pickle', 'rb') as f:\n",
    "    corpus2id_fr = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 10000\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus2id_en), len(corpus2id_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6330 7199\n"
     ]
    }
   ],
   "source": [
    "with open('unigram_probs_hansard_en_subs.p', 'rb') as f:\n",
    "    unigram_en = pickle.load(f)\n",
    "    \n",
    "with open('unigram_probs_hansard_fr_subs.p', 'rb') as f:\n",
    "    unigram_fr = pickle.load(f)\n",
    "    \n",
    "vocabulary_size_en = len(unigram_en)\n",
    "vocabulary_size_fr = len(unigram_fr)\n",
    "print(vocabulary_size_en, vocabulary_size_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sentence length EN: 185\n",
      "Maximum sentence length FR: 275\n"
     ]
    }
   ],
   "source": [
    "max_s_len_en = 0\n",
    "max_s_len_fr = 0\n",
    "\n",
    "\n",
    "for s in range(len(corpus2id_en)):\n",
    "    \n",
    "    if len(corpus2id_en[s]) > max_s_len_en:\n",
    "        max_s_len_en = len(corpus2id_en[s])\n",
    "        \n",
    "    if len(corpus2id_fr[s]) > max_s_len_fr:\n",
    "        max_s_len_fr = len(corpus2id_fr[s])\n",
    "        \n",
    "print('Maximum sentence length EN:', max_s_len_en)\n",
    "print('Maximum sentence length FR:', max_s_len_fr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_batches_EA(corpus_en, vocabulary_size_en, corpus_fr, vocabulary_size_fr, max_sentence_length_en,max_sentence_length_fr, batch_size):\n",
    "    \n",
    "    print(max_sentence_length_en, max_sentence_length_fr)\n",
    "    batches_en = []\n",
    "    batches_fr = []\n",
    "    \n",
    "    no_sentences = len(corpus_en)\n",
    "    \n",
    "    indices = np.arange(0, no_sentences)\n",
    "        \n",
    "    #shuffle set\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for bn in range(no_sentences):\n",
    "        \n",
    "        b_indices = indices[bn*batch_size:bn*batch_size + batch_size]\n",
    "        \n",
    "        batch_max_en = 0\n",
    "        batch_max_fr= 0\n",
    "        \n",
    "        for d in b_indices:\n",
    "            sent_len = len(corpus_en[d])\n",
    "            \n",
    "            if sent_len > batch_max_en:\n",
    "                batch_max_en = sent_len\n",
    "                \n",
    "            sent_len = len(corpus_fr[d])\n",
    "            \n",
    "            if sent_len > batch_max_fr:\n",
    "                batch_max_fr = sent_len\n",
    "                \n",
    "        sentence_b_en = []\n",
    "        sentence_b_fr = []\n",
    "        \n",
    "        for d in b_indices:\n",
    "            \n",
    "            sent_en = corpus_en[d]\n",
    "            \n",
    "#             dif = batch_max_en - len(sent_en)\n",
    "            \n",
    "#             for wd in range(dif):\n",
    "#                 sent_en.append(vocabulary_size_en)\n",
    "                \n",
    "            sent_fr = corpus_fr[d]\n",
    "            \n",
    "#             dif = batch_max_fr - len(sent_fr)\n",
    "            \n",
    "#             for wd in range(dif):\n",
    "#                 sent_fr.append(vocabulary_size_fr)\n",
    "                \n",
    "#             sentence_b_en.append(sent_en)\n",
    "        \n",
    "#             sentence_b_fr.append(sent_fr)\n",
    "    \n",
    "            \n",
    "        batches_en.append(sent_en)\n",
    "        \n",
    "        batches_fr.append(sent_fr)\n",
    "    \n",
    "    return batches_en, batches_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185 275\n"
     ]
    }
   ],
   "source": [
    "batches_en, batches_fr = create_batches_EA(corpus2id_en, vocabulary_size_en, corpus2id_fr, vocabulary_size_fr, max_s_len_en,max_s_len_fr, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batches_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch, total loss, average loss, duration\n",
      "0 385799.56449508667 38.5799564495 0:02:02.143863\n",
      "1 385177.9482564926 38.5488756376 0:03:06.385388\n"
     ]
    }
   ],
   "source": [
    "class EmbedAlign(nn.Module):\n",
    "    def __init__(self, vocab_size_en,vocab_size_fr, embedding_dim):\n",
    "        super(EmbedAlign, self).__init__()\n",
    "        \n",
    "        self.vocab_size_en = vocab_size_en\n",
    "        self.vocab_size_fr = vocab_size_fr\n",
    "        self.embedding_dim = embedding_dim\n",
    "        #for the inference model\n",
    "        self.w_embeddings = nn.Embedding(self.vocab_size_en, self.embedding_dim)\n",
    "        #encoder        \n",
    "        self.bidirLSTM = nn.LSTM(self.embedding_dim, self.embedding_dim, bidirectional=True)\n",
    "        #h_i = hi< + hi>\n",
    "        \n",
    "        self.mu_1 = nn.Linear(self.embedding_dim, self.embedding_dim)\n",
    "        self.mu_2 = nn.Linear(self.embedding_dim, self.embedding_dim)\n",
    "            \n",
    "        self.sigma_1 = nn.Linear(self.embedding_dim, self.embedding_dim)\n",
    "        self.sigma_2 = nn.Linear(self.embedding_dim, self.embedding_dim)\n",
    "        \n",
    "        #for the generative model\n",
    "        self.affine1L1 = nn.Linear(self.embedding_dim, self.embedding_dim)\n",
    "        self.affine2L1 = nn.Linear(self.embedding_dim, self.vocab_size_en)\n",
    "        \n",
    "        self.affine1L2 = nn.Linear(self.embedding_dim, self.embedding_dim)\n",
    "        self.affine2L2 = nn.Linear(self.embedding_dim, self.vocab_size_fr)\n",
    "       \n",
    "        self.dist_norm = torch.distributions.multivariate_normal.MultivariateNormal(torch.ones(self.embedding_dim),torch.diag(torch.ones(self.embedding_dim)))\n",
    "            \n",
    "    def forward(self, batch_en, batch_fr, mu_i, sigma_i, z_i, best_alignments):\n",
    "        \n",
    "        kl_score = 0.0\n",
    "        sent_logx = 0.0\n",
    "        sent_logy = 0.0\n",
    "        \n",
    "        m = len(batch_en)\n",
    "        \n",
    "        for x in range(len(batch_en)):\n",
    "            \n",
    "            word_x = batch_en[x]\n",
    "            embeddings = self.w_embeddings(word_x)\n",
    "            #view_shape = embeddings.shape[0]\n",
    "            output, (hidden, cell) = self.bidirLSTM(embeddings.view(1, 1, -1)) \n",
    "\n",
    "            hid_f = hidden[0]\n",
    "            hid_b = hidden[1]\n",
    "\n",
    "            conc_hids = hid_f + hid_b\n",
    "            \n",
    "            mu = self.mu_1(conc_hids.squeeze())\n",
    "            mu = F.relu(mu)\n",
    "            mu = self.mu_2(mu)\n",
    "\n",
    "            sigma = self.sigma_1(conc_hids.squeeze())\n",
    "            sigma = F.relu(sigma)\n",
    "            sigma = self.sigma_2(sigma)\n",
    "            sigma = F.softplus(sigma)\n",
    "\n",
    "            mu_i.append((word_x, mu))\n",
    "            sigma_i.append((word_x,sigma))\n",
    "            \n",
    "            epsilon = torch.distributions.multivariate_normal.MultivariateNormal(torch.zeros(self.embedding_dim),torch.diag(torch.ones(self.embedding_dim))).sample()\n",
    "\n",
    "            #reparameterize\n",
    "            zi = mu + epsilon * sigma\n",
    "            z_i.append((word_x, zi))\n",
    "            \n",
    "            #generative using sampled zi\n",
    "            #variational location and scale\n",
    "            #same zi for x and y\n",
    "\n",
    "            xi = self.affine1L1(zi)\n",
    "            xi = F.relu(xi)\n",
    "            xi = self.affine2L1(xi)\n",
    "            xi = F.log_softmax(xi, dim=0) #cat generation - target\n",
    "\n",
    "            yi = self.affine1L2(zi)\n",
    "            yi = F.relu(yi)\n",
    "            yi = self.affine2L2(yi)\n",
    "            yi = F.log_softmax(yi, dim=0) #cat generation - source\n",
    "\n",
    "            sent_logx += xi[word_x]\n",
    "            \n",
    "            kl_loss = -(1 + torch.log(sigma**2) - mu ** 2 - sigma**2)/2\n",
    "            \n",
    "            kl_score += kl_loss\n",
    "            \n",
    "            best_j = 0\n",
    "            best_prob = 0\n",
    "            for y in range(len(batch_fr)):\n",
    "                word_y = batch_fr[y]\n",
    "                y_prob = yi[word_y].data\n",
    "                exp_y_prob = np.exp(y_prob)\n",
    "                \n",
    "                if exp_y_prob > best_prob:\n",
    "                    best_prob = exp_y_prob\n",
    "                    best_j = y \n",
    "                    best_y_prob = y_prob\n",
    "                    \n",
    "            best_alignments.append((x,best_j))\n",
    "                    \n",
    "            sent_logy += (-torch.Tensor([np.log(m)])) + best_y_prob\n",
    "                \n",
    "        final_out = -sent_logx - sent_logy + torch.sum(kl_score)\n",
    "            \n",
    "        return final_out, mu_i, sigma_i, xi,yi,z_i, best_alignments\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "model = EmbedAlign(vocabulary_size_en, vocabulary_size_fr, embedding_dim) #pad\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "losses = []\n",
    "avg_losses = []\n",
    "\n",
    "portion = 1000\n",
    "\n",
    "print('epoch, total loss, average loss, duration')\n",
    "for e in range(epochs):\n",
    "    \n",
    "    then = datetime.now()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    for b in range(portion):\n",
    "\n",
    "        mu_i = []\n",
    "        sigma_i = []\n",
    "        z_i = []\n",
    "        best_alignments = []\n",
    "        \n",
    "        if len(batches_en[b]) > 0 and len(batches_fr[b]) > 0:\n",
    "            batch_en = torch.tensor(np.asarray(batches_en[b]), dtype= torch.long)\n",
    "\n",
    "            batch_fr = torch.tensor(np.asarray(batches_fr[b]), dtype= torch.long)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss, mu_i, sigma_i, xi,yi,z_i, best_alignments = model(batch_en, batch_fr,mu_i, sigma_i, z_i, best_alignments)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() \n",
    "    \n",
    "    now = datetime.now()\n",
    "        \n",
    "    losses.append(total_loss)\n",
    "    \n",
    "    avg_loss = np.mean(losses)/len(batches_en)\n",
    "    \n",
    "    print(e, total_loss, avg_loss, now-then)\n",
    "    \n",
    "    avg_losses.append(avg_loss)\n",
    "    \n",
    "with open('mu_' + str(e) + '.pickle', 'wb') as file:\n",
    "    pickle.dump(mu_i, file)\n",
    "with open('sigma_' + str(e) + '.pickle', 'wb') as file:\n",
    "    pickle.dump(sigma_i, file)\n",
    "with open('xi_' + str(e) + '.pickle', 'wb') as file:\n",
    "    pickle.dump(xi, file)\n",
    "with open('yi_' + str(e) + '.pickle', 'wb') as file:\n",
    "    pickle.dump(yi, file)\n",
    "\n",
    "with open('model_embed.pickle','wb') as file:\n",
    "    pickle.dump(model,file)\n",
    "      \n",
    "iteration= list(range(len(losses)))\n",
    "\n",
    "plt.plot(iteration, losses)\n",
    "plt.xlabel(\"Iterations for Embed-Align\")\n",
    "plt.ylabel('Average loss')\n",
    "plt.title('Evolution of the loss as a function of the iteration')\n",
    "plt.savefig(\"embed\" + str(portion)+\".png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6330"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.w_embeddings.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor(651), tensor(1.00000e-02 *\n",
       "         [-0.0394, -0.1702,  0.1654, -0.1143, -0.0795, -0.1322, -0.3883,\n",
       "          -0.3102,  0.3268,  1.6856, -0.7363, -0.1398,  0.3666,  0.5630,\n",
       "          -0.5523, -0.4995,  0.4018,  1.1520, -0.1252,  0.3668,  0.1545,\n",
       "           1.1566,  0.9851, -0.2536, -0.5391, -0.8794, -0.1441, -0.2989,\n",
       "           0.4453, -0.6884,  0.1933, -0.7992, -0.4818,  0.3102,  0.2419,\n",
       "           0.5429, -0.9620,  1.2906, -1.2601, -0.3279, -0.0656, -0.1035,\n",
       "          -0.4948,  0.5398, -0.8085,  0.4382, -0.6078,  0.5787,  0.5502,\n",
       "           0.9391, -0.3852, -0.2138, -1.1644,  0.1110,  0.0059,  0.1697,\n",
       "           0.8674, -0.9582, -0.0120, -1.0850, -1.3202,  0.0765, -0.5263,\n",
       "          -0.3033, -0.4428,  0.4801,  0.2740,  0.1823, -0.0122,  0.1252,\n",
       "          -0.7744,  0.3290,  0.7766, -0.8069,  0.6963, -1.2154, -1.1400,\n",
       "           0.2774,  0.0279, -0.5948, -0.7281, -0.6546,  1.1699,  0.8095,\n",
       "          -0.3029, -0.0841,  0.8815, -0.0464, -0.6955,  0.6826, -0.5940,\n",
       "          -0.2968, -0.6845, -0.4444, -0.1107, -0.0377, -0.1836,  1.4315,\n",
       "           0.5043,  0.0570])), (tensor(3152), tensor(1.00000e-02 *\n",
       "         [-0.0394, -0.1702,  0.1654, -0.1143, -0.0795, -0.1322, -0.3883,\n",
       "          -0.3102,  0.3268,  1.6856, -0.7363, -0.1398,  0.3666,  0.5630,\n",
       "          -0.5523, -0.4995,  0.4018,  1.1520, -0.1252,  0.3668,  0.1545,\n",
       "           1.1566,  0.9851, -0.2536, -0.5391, -0.8794, -0.1441, -0.2989,\n",
       "           0.4453, -0.6884,  0.1933, -0.7992, -0.4818,  0.3102,  0.2419,\n",
       "           0.5429, -0.9620,  1.2906, -1.2601, -0.3279, -0.0656, -0.1035,\n",
       "          -0.4948,  0.5398, -0.8085,  0.4382, -0.6078,  0.5787,  0.5502,\n",
       "           0.9391, -0.3852, -0.2138, -1.1644,  0.1110,  0.0059,  0.1697,\n",
       "           0.8674, -0.9582, -0.0120, -1.0850, -1.3202,  0.0765, -0.5263,\n",
       "          -0.3033, -0.4428,  0.4801,  0.2740,  0.1823, -0.0122,  0.1252,\n",
       "          -0.7744,  0.3290,  0.7766, -0.8069,  0.6963, -1.2154, -1.1400,\n",
       "           0.2774,  0.0279, -0.5948, -0.7281, -0.6546,  1.1699,  0.8095,\n",
       "          -0.3029, -0.0841,  0.8815, -0.0464, -0.6955,  0.6826, -0.5940,\n",
       "          -0.2968, -0.6845, -0.4444, -0.1107, -0.0377, -0.1836,  1.4315,\n",
       "           0.5043,  0.0570]))]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
