{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import spatial\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "from random import randint\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_count = 2\n",
    "threshold_for_subsampling = 0.00001 #recommended in Mikolov 2013\n",
    "neg_context_k = 5 #negative samples\n",
    "embedding_dim = 100\n",
    "batch_size = 50\n",
    "epochs = 10\n",
    "lr = 0.01\n",
    "window_size = 5 #dynamic given this value\n",
    "\n",
    "#do we include punctuation marks?\n",
    "#what to do about stop words, function words?\n",
    "#what to do about infrequent words?\n",
    "#what to do about numbers?\n",
    "#what should be the output? which candidates should be selected?\n",
    "#we removed \\n\n",
    "#what to do for word embedding initialization\n",
    "# Improving Distributional Similarity with Lessons Learned fromWord Embeddings\n",
    "# Omer Levy Yoav Goldberg Ido Dagan Computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size 322\n",
      "After rare word pruning 84\n"
     ]
    }
   ],
   "source": [
    "dataset = 'wa/dev.en'\n",
    "\n",
    "#read the files\n",
    "with open(dataset) as f:\n",
    "    sentences = [l.strip() for l in f.readlines()]\n",
    "\n",
    "#get all the tokens from the corpus\n",
    "\n",
    "tokens_list = []\n",
    "sentence_list = []\n",
    "for s in sentences:\n",
    "    split_sent = s.split()\n",
    "    sentence = []\n",
    "    for w in split_sent:\n",
    "        \n",
    "        tokens_list.append(w)\n",
    "        sentence.append(w)\n",
    "#         #filter stopwords\n",
    "#         if w not in stopwords:\n",
    "#             tokens_list.append(w)\n",
    "#             sentence.append(w)\n",
    "    \n",
    "    sentence_list.append(sentence)\n",
    "    \n",
    "tokens = list(sorted(set(tokens_list)))\n",
    "print('Vocabulary size', len(tokens))\n",
    "\n",
    "count_tokens = Counter(tokens_list)\n",
    "\n",
    "#words appearing fewer than this are not considered words or contexts\n",
    "#subsample frequent words\n",
    "\n",
    "temp_sentence_list = []\n",
    "\n",
    "for s in range(len(sentence_list)):\n",
    "    temp_sentence_list.append(sentence_list[s])\n",
    "\n",
    "#Find the infrequent words\n",
    "for s in range(len(temp_sentence_list)):\n",
    "    for w in range(len(temp_sentence_list[s])):\n",
    "        word = temp_sentence_list[s][w]\n",
    "        if count_tokens[word] < min_count:\n",
    "            sentence_list[s][w] = '<unk>'\n",
    "            \n",
    "            #remove the infrequent words from tokens\n",
    "            tokens.remove(word)\n",
    "\n",
    "#we prune rare words altogether\n",
    "\n",
    "#tokens.append('<unk>')\n",
    "vocab_size = len(tokens)\n",
    "print('After rare word pruning', vocab_size)\n",
    "#remove the infrequent words from the sentences\n",
    "for s in sentence_list:\n",
    "    s[:] = [w for w in s if w != '<unk>']\n",
    "\n",
    "flat_token_list = []\n",
    "\n",
    "for s in sentence_list:\n",
    "    for w in s:\n",
    "        flat_token_list.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'.': 34, 'the': 33, 'to': 23, ',': 17, 'of': 14, 'in': 14, 'and': 13, 'that': 12, 'for': 11, 'it': 11, 'we': 11, 'a': 10, 'have': 9, 'is': 7, 'they': 7, 'be': 7, 'are': 6, 'I': 6, 'as': 6, 'very': 5, 'not': 5, 'our': 5, 'been': 5, 'this': 5, '$': 4, 'do': 4, 'what': 4, 'them': 3, 'which': 3, 'people': 3, 'on': 3, 'by': 3, 'one': 3, 'does': 3, 'Bill': 3, '-': 3, 'has': 3, 'there': 3, 'public': 3, 'or': 3, 'at': 3, 'however': 3, 'would': 3, 'want': 3, 'no': 3, 'will': 3, 'Mr.': 3, 'women': 3, 'complex': 2, 'whole': 2, 'most': 2, 'today': 2, 'who': 2, 'effective': 2, 'so': 2, 'let': 2, 'think': 2, 'should': 2, 'look': 2, 'cost': 2, 'some': 2, 'given': 2, 'see': 2, 'up': 2, 'say': 2, 'than': 2, 'programs': 2, 'within': 2, 'country': 2, 'support': 2, '?': 2, 'make': 2, 'Prime': 2, 'Minister': 2, 'need': 2, 'action': 2, 'under': 2, 'Speaker': 2, 'pretends': 2, 'employment': 2, 'equity': 2, 'job': 2, 'growers': 2, 'begin': 2})\n",
      "423\n"
     ]
    }
   ],
   "source": [
    "subsample_probs = defaultdict(float) \n",
    "\n",
    "count_words = Counter(flat_token_list)\n",
    "print(count_words)\n",
    "print(sum(count_words.values()))\n",
    "\n",
    "for t in count_words:\n",
    "    \n",
    "    #Distributed Representations of Words and Phrases and their Compositionality\n",
    "    #Mikolov 2013\n",
    "    #p = 1 - sqrt(t/f)\n",
    "    subsample_probs[t] = 1 - np.sqrt(threshold_for_subsampling/(count_words[t]/len(flat_sentence_list)))\n",
    "    \n",
    "temp_sentence_list = []\n",
    "\n",
    "for s in range(len(sentence_list)):\n",
    "    temp_sentence_list.append(sentence_list[s])\n",
    "\n",
    "subsampled_sentence_list = []\n",
    "\n",
    "#Prune frequent words\n",
    "for s in range(len(temp_sentence_list)):\n",
    "    sent = []\n",
    "    for w in range(len(temp_sentence_list[s])):\n",
    "        \n",
    "        word = temp_sentence_list[s][w]\n",
    "        word_prob = subsample_probs[word]\n",
    "        \n",
    "        rand_prob = np.random.rand(1)[0]\n",
    "       \n",
    "        if rand_prob < word_prob:\n",
    "            #keep word\n",
    "            sent.append(temp_sentence_list[s][w])\n",
    "    \n",
    "    subsampled_sentence_list.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(subsampled_sentence_list, open('subsampled_dataset.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subsampled_sentence_list = pickle.load(open('subsampled_dataset.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_subsampled_token_list = []\n",
    "\n",
    "for s in subsampled_sentence_list:\n",
    "    for w in s:\n",
    "        flat_subsampled_token_list.append(w)\n",
    "\n",
    "unigram_counts = Counter(flat_subsampled_token_list)\n",
    "\n",
    "unigram_probs = defaultdict(float) \n",
    "\n",
    "for t in unigram_counts:\n",
    "    unigram_probs[t] = unigram_counts[t]/len(flat_subsampled_token_list)\n",
    "\n",
    "pickle.dump(unigram_probs, open('unigram_probs.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_probs = pickle.load(open('unigram_probs.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sampled_window(sentence, central_word_index, window_size):\n",
    "    \n",
    "    #implicit weighing of the context words\n",
    "    #dynamic context window size for each word\n",
    "    dynamic_window_size = randint(1,window_size)\n",
    "    \n",
    "    #dws = 2, i-2,i-1,i+1,i+2\n",
    "    \n",
    "    pairs = []\n",
    "    word = sentence[central_word_index]\n",
    "    for w in range(1,dynamic_window_size+1):\n",
    "        \n",
    "        left_cont = central_word_index - w\n",
    "        right_cont = central_word_index + w\n",
    "        \n",
    "        #find the window words to the left and right\n",
    "        #add as pair if they are inside sentence boundaries\n",
    "        \n",
    "        if left_cont > -1:\n",
    "            pairs.append((word, sentence[left_cont]))\n",
    "        \n",
    "        if right_cont < len(sentence):\n",
    "            pairs.append((word, sentence[right_cont]))\n",
    "        \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_sample_probs = defaultdict(float)\n",
    "\n",
    "neg_normalizer = 0.0\n",
    "\n",
    "for t in unigram_counts:\n",
    "    neg_sample_probs[t] = np.power(unigram_counts[t], 0.75)\n",
    "    neg_normalizer += neg_sample_probs[t]\n",
    "    \n",
    "\n",
    "for n in neg_sample_probs:\n",
    "    neg_sample_probs[n] = neg_sample_probs[n] / neg_normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999973\n",
      "9999973\n",
      "['of' 'the' 'should' 'to' 'as' 'what' 'within' 'public' 'would' 'it']\n"
     ]
    }
   ],
   "source": [
    "#Ã  la Mikolov\n",
    "#table to pick negative context words\n",
    "#fill the table with each word, count = negsampling prob*table size\n",
    "neg_table_size = 10000000\n",
    "\n",
    "unigram_table = np.empty(neg_table_size, dtype=object)\n",
    "i = 0\n",
    "\n",
    "for n in neg_sample_probs:\n",
    "    count = int(neg_sample_probs[n] * neg_table_size)\n",
    "    \n",
    "    for c in range(count):\n",
    "        unigram_table[i] = n\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "unigram_table = unigram_table[unigram_table != np.array(None)]\n",
    "np.random.shuffle(unigram_table)\n",
    "print(i)\n",
    "print(len(unigram_table))\n",
    "print(unigram_table[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_neg_context(table, k):\n",
    "    \n",
    "    neg_samples = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        \n",
    "        r = randint(0,len(table)-1)\n",
    "        neg_samples.append(table[r])\n",
    "        \n",
    "    return neg_samples    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_context_pairs = []\n",
    "neg_context_samples = []\n",
    "\n",
    "for sentence in sentence_list:\n",
    "    for w in range(len(sentence)):\n",
    "        pos_context_pairs.extend(sampled_window(sentence, w, window_size))\n",
    "        \n",
    "        neg_context_samples.append((sentence[w], pick_neg_context(unigram_table, neg_context_k)))\n",
    "\n",
    "pickle.dump(pos_context_pairs, open('pos_context_pairs.p', 'wb'))\n",
    "pickle.dump(neg_context_samples, open('neg_context_samples.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_context_pairs = pickle.load(open('pos_context_pairs.p', 'rb'))\n",
    "neg_context_samples = pickle.load(open('neg_context_samples.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n"
     ]
    }
   ],
   "source": [
    "tokens = list(set(flat_subsampled_token_list))\n",
    "\n",
    "#default dictionary key:id value:token\n",
    "id2tokens = defaultdict(str)\n",
    "                        \n",
    "for i in range(len(tokens)):\n",
    "    id2tokens[i] = tokens[i]\n",
    "    \n",
    "#default dictionary key:token value:id\n",
    "tokens2id = defaultdict(int)\n",
    "\n",
    "for ind in id2tokens:\n",
    "    tokens2id[id2tokens[ind]] = ind\n",
    "    \n",
    "vocabulary_size = len(tokens2id)\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dataset to ids\n",
    "pos_data = []\n",
    "neg_data = []\n",
    "\n",
    "for p in pos_context_pairs:\n",
    "    pos_data.append((tokens2id[p[0]], tokens2id[p[1]]))\n",
    "    \n",
    "for n in neg_context_samples:\n",
    "    \n",
    "    word_id = tokens2id[n[0]]\n",
    "    neg_samples = n[1]\n",
    "    \n",
    "    neg_ids = []\n",
    "    \n",
    "    for ns in neg_samples:\n",
    "        neg_ids.append(tokens2id[ns])\n",
    "        \n",
    "    neg_data.append((word_id, neg_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41, 64) ('of', 'them') ('of', ['it', '?', 'have', 'the', 'and']) (41, [44, 47, 19, 10, 79])\n"
     ]
    }
   ],
   "source": [
    "print(pos_data[0], pos_context_pairs[0], neg_context_samples[0], neg_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(pos_data, open('pos_data.p', 'wb'))\n",
    "pickle.dump(neg_data, open('neg_data.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_data = pickle.load(open('pos_data.p', 'rb'))\n",
    "neg_data = pickle.load(open('neg_data.p', 'rb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
