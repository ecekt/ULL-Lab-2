{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import spatial\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "from random import randint\n",
    "import pickle\n",
    "import nltk\n",
    "import string\n",
    "puncs = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_count = 2\n",
    "threshold_for_subsampling = 0.00001 #recommended in Mikolov 2013\n",
    "neg_context_k = 5 #negative samples\n",
    "embedding_dim = 100\n",
    "batch_size = 50\n",
    "epochs = 10\n",
    "lr = 0.01\n",
    "window_size = 5 #dynamic given this value\n",
    "\n",
    "#removed punctuation marks, removed infrequent ones, subsampled frequent ones\n",
    "#negative samples done \n",
    "\n",
    "#what should be the output? which candidates should be selected?\n",
    "#what to do for word embedding initialization\n",
    "# Improving Distributional Similarity with Lessons Learned fromWord Embeddings\n",
    "# Omer Levy Yoav Goldberg Ido Dagan Computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sampled_window(sentence, central_word_index, window_size):\n",
    "    \n",
    "    #implicit weighing of the context words\n",
    "    #dynamic context window size for each word\n",
    "    dynamic_window_size = randint(1,window_size)\n",
    "    \n",
    "    #dws = 2, i-2,i-1,i+1,i+2\n",
    "    \n",
    "    pairs = []\n",
    "    word = sentence[central_word_index]\n",
    "    for w in range(1,dynamic_window_size+1):\n",
    "        \n",
    "        left_cont = central_word_index - w\n",
    "        right_cont = central_word_index + w\n",
    "        \n",
    "        #find the window words to the left and right\n",
    "        #add as pair if they are inside sentence boundaries\n",
    "        \n",
    "        if left_cont > -1:\n",
    "            pairs.append((word, sentence[left_cont]))\n",
    "            \n",
    "        if right_cont < len(sentence):\n",
    "            pairs.append((word, sentence[right_cont]))\n",
    "            \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_neg_context(table, k, word, pos_pairs_dict):\n",
    "    \n",
    "    neg_samples = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        \n",
    "        r = randint(0,len(table)-1)\n",
    "        \n",
    "        possible_neg = table[r]\n",
    "        \n",
    "        while possible_neg in pos_pair_dict[word]:\n",
    "            \n",
    "            r = randint(0,len(table)-1)\n",
    "        \n",
    "            possible_neg = table[r]\n",
    "                    \n",
    "        neg_samples.append(possible_neg)\n",
    "        \n",
    "    return neg_samples    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size 322\n",
      "After rare word pruning 84\n"
     ]
    }
   ],
   "source": [
    "# dataset = 'hansards/training.en'\n",
    "# label = '_hansard_en'\n",
    "\n",
    "# dataset = 'hansards/training.fr'\n",
    "# label = '_hansard_fr'\n",
    "\n",
    "dataset = 'wa/dev.en'\n",
    "label = '_dev'\n",
    "\n",
    "#read the files\n",
    "with open(dataset) as f:\n",
    "    sentences = [l.strip() for l in f.readlines()]\n",
    "\n",
    "#get all the tokens from the corpus\n",
    "\n",
    "tokens_list = []\n",
    "sentence_list = []\n",
    "for s in sentences:\n",
    "    split_sent = s.split()\n",
    "    sentence = []\n",
    "    for w in split_sent:\n",
    "        \n",
    "        tokens_list.append(w)\n",
    "        sentence.append(w)\n",
    "#         #filter stopwords\n",
    "#         if w not in stopwords:\n",
    "#             tokens_list.append(w)\n",
    "#             sentence.append(w)\n",
    "    \n",
    "    sentence_list.append(sentence)\n",
    "    \n",
    "tokens = list(sorted(set(tokens_list)))\n",
    "print('Vocabulary size', len(tokens))\n",
    "\n",
    "count_tokens = Counter(tokens_list)\n",
    "\n",
    "#words appearing fewer than this are not considered words or contexts\n",
    "#subsample frequent words\n",
    "\n",
    "temp_sentence_list = []\n",
    "\n",
    "for s in range(len(sentence_list)):\n",
    "    temp_sentence_list.append(sentence_list[s])\n",
    "\n",
    "#Find the infrequent words\n",
    "for s in range(len(temp_sentence_list)):\n",
    "    for w in range(len(temp_sentence_list[s])):\n",
    "        word = temp_sentence_list[s][w]\n",
    "        if count_tokens[word] < min_count:\n",
    "            sentence_list[s][w] = '<unk>'\n",
    "            \n",
    "            #remove the infrequent words from tokens\n",
    "            tokens.remove(word)\n",
    "\n",
    "#we prune rare words altogether\n",
    "\n",
    "#tokens.append('<unk>')\n",
    "vocab_size = len(tokens)\n",
    "print('After rare word pruning', vocab_size)\n",
    "#remove the infrequent words from the sentences\n",
    "for s in sentence_list:\n",
    "    s[:] = [w for w in s if w != '<unk>' and w not in puncs]\n",
    "\n",
    "flat_token_list = []\n",
    "\n",
    "for s in sentence_list:\n",
    "    for w in s:\n",
    "        flat_token_list.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_probs = defaultdict(float) \n",
    "\n",
    "count_words = Counter(flat_token_list)\n",
    "\n",
    "for c in count_words:\n",
    "    \n",
    "    #Distributed Representations of Words and Phrases and their Compositionality\n",
    "    #Mikolov 2013\n",
    "    #p = 1 - sqrt(t/f)\n",
    "    #Levy&Goldberg, actual implementation in code p = (f-t)/f - sqrt(t/f)\n",
    "    \n",
    "    f = count_words[c]/len(flat_token_list)\n",
    "    t = threshold_for_subsampling\n",
    "    #subsample_probs[t] = 1 - np.sqrt(threshold_for_subsampling/(count_words[t]/len(flat_token_list)))\n",
    "    subsample_probs[c] =  (f-t)/f - np.sqrt(t/f)\n",
    "    \n",
    "temp_sentence_list = []\n",
    "\n",
    "for s in range(len(sentence_list)):\n",
    "    temp_sentence_list.append(sentence_list[s])\n",
    "\n",
    "subsampled_sentence_list = []\n",
    "\n",
    "#Prune frequent words\n",
    "for s in range(len(temp_sentence_list)):\n",
    "    sent = []\n",
    "    for w in range(len(temp_sentence_list[s])):\n",
    "        \n",
    "        word = temp_sentence_list[s][w]\n",
    "        word_prob = subsample_probs[word]\n",
    "        \n",
    "        rand_prob = np.random.rand(1)[0]\n",
    "       \n",
    "        if rand_prob < word_prob:\n",
    "            #keep word\n",
    "            sent.append(temp_sentence_list[s][w])\n",
    "    \n",
    "    subsampled_sentence_list.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(subsampled_sentence_list, open('subsampled_dataset' + label + '.p', 'wb'))\n",
    "\n",
    "subsampled_sentence_list = pickle.load(open('subsampled_dataset' + label + '.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_subsampled_token_list = []\n",
    "\n",
    "for s in subsampled_sentence_list:\n",
    "    for w in s:\n",
    "        flat_subsampled_token_list.append(w)\n",
    "\n",
    "unigram_counts = Counter(flat_subsampled_token_list)\n",
    "\n",
    "unigram_probs = defaultdict(float) \n",
    "\n",
    "for t in unigram_counts:\n",
    "    unigram_probs[t] = unigram_counts[t]/len(flat_subsampled_token_list)\n",
    "\n",
    "pickle.dump(unigram_probs, open('unigram_probs' + label + '.p', 'wb'))\n",
    "\n",
    "unigram_probs = pickle.load(open('unigram_probs' + label + '.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_sample_probs = defaultdict(float)\n",
    "\n",
    "neg_normalizer = 0.0\n",
    "\n",
    "for t in unigram_probs:\n",
    "    neg_sample_probs[t] = np.power(unigram_probs[t], 0.75)\n",
    "    neg_normalizer += neg_sample_probs[t]\n",
    "    \n",
    "for n in neg_sample_probs:\n",
    "    neg_sample_probs[n] = neg_sample_probs[n] / neg_normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999953\n",
      "9999953\n",
      "['of' 'of' 'not' 'to' 'under' 'say' 'not' 'have' 'has' 'no']\n"
     ]
    }
   ],
   "source": [
    "#Ã  la Mikolov\n",
    "#table to pick negative context words\n",
    "#fill the table with each word, count = negsampling prob*table size\n",
    "neg_table_size = 10000000\n",
    "\n",
    "unigram_table = np.empty(neg_table_size, dtype=object)\n",
    "i = 0\n",
    "\n",
    "for n in neg_sample_probs:\n",
    "    count = int(neg_sample_probs[n] * neg_table_size)\n",
    "    \n",
    "    for c in range(count):\n",
    "        unigram_table[i] = n\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "unigram_table = unigram_table[unigram_table != np.array(None)]\n",
    "np.random.shuffle(unigram_table)\n",
    "print(i)\n",
    "print(len(unigram_table))\n",
    "print(unigram_table[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_context_pairs = []\n",
    "neg_context_samples = []\n",
    "\n",
    "for sentence in sentence_list:\n",
    "    for w in range(len(sentence)):\n",
    "        \n",
    "        pos_context_pairs.extend(sampled_window(sentence, w, window_size))\n",
    "\n",
    "\n",
    "pickle.dump(pos_context_pairs, open('pos_context_pairs' + label + '.p', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_pair_dict = defaultdict(set)\n",
    "        \n",
    "for pair in pos_context_pairs:\n",
    "    pos_pair_dict[pair[0]].add(pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('pos_pair_dict' + label + '.p', 'wb') as f:\n",
    "    pickle.dump(pos_pair_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('pos_pair_dict' + label + '.p', 'rb') as f:\n",
    "    pos_pair_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in range(len(pos_context_pairs)):\n",
    "    \n",
    "    pair = pos_context_pairs[p]\n",
    "    neg_context_samples.append((pair[0], pick_neg_context(unigram_table, neg_context_k, pair[0], pos_pair_dict)))\n",
    "    \n",
    "pickle.dump(neg_context_samples, open('neg_context_samples' + label + '.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1690 1690\n",
      "('of', 'them') ('of', ['Prime', 'Prime', 'a', 'Prime', 'Bill'])\n"
     ]
    }
   ],
   "source": [
    "print(len(pos_context_pairs), len(neg_context_samples))\n",
    "print(pos_context_pairs[0], neg_context_samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_context_pairs = pickle.load(open('pos_context_pairs' + label + '.p', 'rb'))\n",
    "neg_context_samples = pickle.load(open('neg_context_samples' + label + '.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n"
     ]
    }
   ],
   "source": [
    "tokens = list(set(flat_subsampled_token_list))\n",
    "\n",
    "#default dictionary key:id value:token\n",
    "id2tokens = defaultdict(str)\n",
    "                        \n",
    "for i in range(len(tokens)):\n",
    "    id2tokens[i] = tokens[i]\n",
    "    \n",
    "#default dictionary key:token value:id\n",
    "tokens2id = defaultdict(int)\n",
    "\n",
    "for ind in id2tokens:\n",
    "    tokens2id[id2tokens[ind]] = ind\n",
    "    \n",
    "vocabulary_size = len(tokens2id)\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert dataset to ids\n",
    "pos_data = []\n",
    "neg_data = []\n",
    "\n",
    "for p in pos_context_pairs:\n",
    "    pos_data.append((tokens2id[p[0]], tokens2id[p[1]]))\n",
    "    \n",
    "for n in neg_context_samples:\n",
    "    \n",
    "    word_id = tokens2id[n[0]]\n",
    "    neg_samples = n[1]\n",
    "    \n",
    "    neg_ids = []\n",
    "    \n",
    "    for ns in neg_samples:\n",
    "        neg_ids.append(tokens2id[ns])\n",
    "        \n",
    "    neg_data.append((word_id, neg_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1690 1690 1690 1690\n"
     ]
    }
   ],
   "source": [
    "print(len(pos_data), len(neg_data), len(pos_context_pairs), len(neg_context_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37, 33) ('of', 'them') ('of', ['Prime', 'Prime', 'a', 'Prime', 'Bill']) (37, [71, 71, 16, 71, 27])\n"
     ]
    }
   ],
   "source": [
    "print(pos_data[0], pos_context_pairs[0], neg_context_samples[0], neg_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(pos_data, open('pos_data' + label + '.p', 'wb'))\n",
    "pickle.dump(neg_data, open('neg_data' + label + '.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('pos_data' + label + '.p', 'rb') as f:\n",
    "    pos_data = pickle.load(f)\n",
    "    \n",
    "with open('neg_data' + label + '.p', 'rb') as f:\n",
    "    neg_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
