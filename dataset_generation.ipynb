{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import spatial\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "from random import randint\n",
    "import pickle\n",
    "import nltk\n",
    "import string\n",
    "puncs = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_count = 20\n",
    "threshold_for_subsampling = 0.00001 #recommended in Mikolov 2013\n",
    "neg_context_k = 5 #negative samples\n",
    "embedding_dim = 100\n",
    "batch_size = 100\n",
    "window_size = 5 #dynamic given this value\n",
    "\n",
    "#removed punctuation marks, removed infrequent ones, subsampled frequent ones\n",
    "#negative samples done \n",
    "\n",
    "#what should be the output? which candidates should be selected?\n",
    "#what to do for word embedding initialization\n",
    "# Improving Distributional Similarity with Lessons Learned fromWord Embeddings\n",
    "# Omer Levy Yoav Goldberg Ido Dagan Computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sampled_window(sentence, central_word_index, window_size):\n",
    "    \n",
    "    #implicit weighing of the context words\n",
    "    #dynamic context window size for each word\n",
    "    dynamic_window_size = randint(1,window_size)\n",
    "    \n",
    "    #dws = 2, i-2,i-1,i+1,i+2\n",
    "    \n",
    "    pairs = []\n",
    "    word = sentence[central_word_index]\n",
    "    for w in range(1,dynamic_window_size+1):\n",
    "        \n",
    "        left_cont = central_word_index - w\n",
    "        right_cont = central_word_index + w\n",
    "        \n",
    "        #find the window words to the left and right\n",
    "        #add as pair if they are inside sentence boundaries\n",
    "        \n",
    "        if left_cont > -1:\n",
    "            pairs.append((word, sentence[left_cont]))\n",
    "            \n",
    "        if right_cont < len(sentence):\n",
    "            pairs.append((word, sentence[right_cont]))\n",
    "            \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_neg_context(table, k, word, pos_pairs_dict):\n",
    "    \n",
    "    neg_samples = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        \n",
    "        r = randint(0,len(table)-1)\n",
    "        \n",
    "        possible_neg = table[r]\n",
    "        \n",
    "        while possible_neg in pos_pair_dict[word]:\n",
    "            \n",
    "            r = randint(0,len(table)-1)\n",
    "        \n",
    "            possible_neg = table[r]\n",
    "                    \n",
    "        neg_samples.append(possible_neg)\n",
    "        \n",
    "    return neg_samples    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size 36635\n"
     ]
    }
   ],
   "source": [
    "# dataset = 'europarl/training.en'\n",
    "# label = '_europarl_en'\n",
    "\n",
    "# dataset = 'europarl/training.fr'\n",
    "# label = '_europarl_fr'\n",
    "\n",
    "dataset = 'hansards/training.en'\n",
    "label = '_hansard_en'\n",
    "\n",
    "# dataset = 'hansards/training.fr'\n",
    "# label = '_hansard_fr'\n",
    "\n",
    "# dataset = 'wa/dev.en'\n",
    "# label = '_dev'\n",
    "\n",
    "#read the files\n",
    "with open(dataset) as f:\n",
    "    sentences = [l.strip() for l in f.readlines()]\n",
    "\n",
    "#get all the tokens from the corpus\n",
    "\n",
    "tokens_list = []\n",
    "sentence_list = []\n",
    "for s in sentences:\n",
    "    split_sent = s.split()\n",
    "    sentence = []\n",
    "    for w in split_sent:\n",
    "        \n",
    "        tokens_list.append(w)\n",
    "        sentence.append(w)\n",
    "#         #filter stopwords\n",
    "#         if w not in stopwords:\n",
    "#             tokens_list.append(w)\n",
    "#             sentence.append(w)\n",
    "    \n",
    "    sentence_list.append(sentence)\n",
    "    \n",
    "tokens = list(sorted(set(tokens_list)))\n",
    "print('Vocabulary size', len(tokens))\n",
    "\n",
    "count_tokens = Counter(tokens_list)\n",
    "\n",
    "#words appearing fewer than this are not considered words or contexts\n",
    "#subsample frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After rare word pruning 7981\n"
     ]
    }
   ],
   "source": [
    "#Find the infrequent words\n",
    "for s in range(len(sentence_list)):\n",
    "    for w in range(len(sentence_list[s])):\n",
    "        word = sentence_list[s][w]\n",
    "        if count_tokens[word] < min_count:\n",
    "            sentence_list[s][w] = '<unk>'\n",
    "            \n",
    "            #remove the infrequent words from tokens\n",
    "            if   word in tokens:\n",
    "                tokens.remove(word)\n",
    "\n",
    "#we prune rare words altogether\n",
    "\n",
    "tokens.append('<unk>')\n",
    "vocab_size = len(tokens)\n",
    "print('After rare word pruning', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove the infrequent words from the sentences\n",
    "for s in sentence_list:\n",
    "    s[:] = [w for w in s if w != '<unk>' and w not in puncs]\n",
    "\n",
    "flat_token_list = []\n",
    "\n",
    "for s in sentence_list:\n",
    "    for w in s:\n",
    "        flat_token_list.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subsample_probs = defaultdict(float) \n",
    "\n",
    "count_words = Counter(flat_token_list)\n",
    "\n",
    "for c in count_words:\n",
    "    \n",
    "    #Distributed Representations of Words and Phrases and their Compositionality\n",
    "    #Mikolov 2013\n",
    "    #p = 1 - sqrt(t/f)\n",
    "    #Levy&Goldberg, actual implementation in code p = (f-t)/f - sqrt(t/f)\n",
    "    \n",
    "    f = count_words[c]/len(flat_token_list)\n",
    "    t = threshold_for_subsampling\n",
    "    #subsample_probs[t] = 1 - np.sqrt(threshold_for_subsampling/(count_words[t]/len(flat_token_list)))\n",
    "    subsample_probs[c] =  (f-t)/f - np.sqrt(t/f)\n",
    "    \n",
    "temp_sentence_list = []\n",
    "\n",
    "for s in range(len(sentence_list)):\n",
    "    temp_sentence_list.append(sentence_list[s])\n",
    "\n",
    "subsampled_sentence_list = []\n",
    "\n",
    "#Prune frequent words\n",
    "for s in range(len(temp_sentence_list)):\n",
    "    sent = []\n",
    "    for w in range(len(temp_sentence_list[s])):\n",
    "        \n",
    "        word = temp_sentence_list[s][w]\n",
    "        word_prob = subsample_probs[word]\n",
    "        \n",
    "        rand_prob = np.random.rand(1)[0]\n",
    "       \n",
    "        if rand_prob < word_prob:\n",
    "            #keep word\n",
    "            sent.append(temp_sentence_list[s][w])\n",
    "    \n",
    "    subsampled_sentence_list.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(subsampled_sentence_list, open('subsampled_dataset' + label + '.p', 'wb'))\n",
    "\n",
    "subsampled_sentence_list = pickle.load(open('subsampled_dataset' + label + '.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flat_subsampled_token_list = []\n",
    "\n",
    "for s in subsampled_sentence_list:\n",
    "    for w in s:\n",
    "        flat_subsampled_token_list.append(w)\n",
    "\n",
    "unigram_counts = Counter(flat_subsampled_token_list)\n",
    "\n",
    "unigram_probs = defaultdict(float) \n",
    "\n",
    "for t in unigram_counts:\n",
    "    unigram_probs[t] = unigram_counts[t]/len(flat_subsampled_token_list)\n",
    "\n",
    "pickle.dump(unigram_probs, open('unigram_probs' + label + '.p', 'wb'))\n",
    "\n",
    "unigram_probs = pickle.load(open('unigram_probs' + label + '.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_sample_probs = defaultdict(float)\n",
    "\n",
    "neg_normalizer = 0.0\n",
    "\n",
    "for t in unigram_probs:\n",
    "    neg_sample_probs[t] = np.power(unigram_probs[t], 0.75)\n",
    "    neg_normalizer += neg_sample_probs[t]\n",
    "    \n",
    "for n in neg_sample_probs:\n",
    "    neg_sample_probs[n] = neg_sample_probs[n] / neg_normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9998286\n",
      "9998286\n",
      "['lives' 'water' 'this' 'negative' 'as' 'any' 'public' 'while' 'created'\n",
      " 'to']\n"
     ]
    }
   ],
   "source": [
    "#Ã  la Mikolov\n",
    "#table to pick negative context words\n",
    "#fill the table with each word, count = negsampling prob*table size\n",
    "neg_table_size = 10000000\n",
    "\n",
    "unigram_table = np.empty(neg_table_size, dtype=object)\n",
    "i = 0\n",
    "\n",
    "for n in neg_sample_probs:\n",
    "    count = int(neg_sample_probs[n] * neg_table_size)\n",
    "    \n",
    "    for c in range(count):\n",
    "        unigram_table[i] = n\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "unigram_table = unigram_table[unigram_table != np.array(None)]\n",
    "np.random.shuffle(unigram_table)\n",
    "print(i)\n",
    "print(len(unigram_table))\n",
    "print(unigram_table[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_context_pairs = []\n",
    "neg_context_samples = []\n",
    "\n",
    "for sentence in sentence_list:\n",
    "    for w in range(len(sentence)):\n",
    "        \n",
    "        pos_context_pairs.extend(sampled_window(sentence, w, window_size))\n",
    "\n",
    "\n",
    "pickle.dump(pos_context_pairs, open('pos_context_pairs' + label + '.p', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_pair_dict = defaultdict(set)\n",
    "        \n",
    "for pair in pos_context_pairs:\n",
    "    pos_pair_dict[pair[0]].add(pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('pos_pair_dict' + label + '.p', 'wb') as f:\n",
    "    pickle.dump(pos_pair_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('pos_pair_dict' + label + '.p', 'rb') as f:\n",
    "    pos_pair_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for p in range(len(pos_context_pairs)):\n",
    "    \n",
    "    pair = pos_context_pairs[p]\n",
    "    neg_context_samples.append((pair[0], pick_neg_context(unigram_table, neg_context_k, pair[0], pos_pair_dict)))\n",
    "    \n",
    "pickle.dump(neg_context_samples, open('neg_context_samples' + label + '.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17127853 17127853\n",
      "('division', '68') ('division', ['bring', 'decide', 'information', 'witnesses', 'Nisga'])\n"
     ]
    }
   ],
   "source": [
    "print(len(pos_context_pairs), len(neg_context_samples))\n",
    "print(pos_context_pairs[0], neg_context_samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_context_pairs = pickle.load(open('pos_context_pairs' + label + '.p', 'rb'))\n",
    "neg_context_samples = pickle.load(open('neg_context_samples' + label + '.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3276\n"
     ]
    }
   ],
   "source": [
    "tokens = list(set(flat_subsampled_token_list))\n",
    "\n",
    "#default dictionary key:id value:token\n",
    "id2tokens = defaultdict(str)\n",
    "                        \n",
    "for i in range(len(tokens)):\n",
    "    id2tokens[i] = tokens[i]\n",
    "    \n",
    "#default dictionary key:token value:id\n",
    "tokens2id = defaultdict(int)\n",
    "\n",
    "for ind in id2tokens:\n",
    "    tokens2id[id2tokens[ind]] = ind\n",
    "    \n",
    "vocabulary_size = len(tokens2id)\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert dataset to ids\n",
    "pos_data = []\n",
    "neg_data = []\n",
    "\n",
    "for p in pos_context_pairs:\n",
    "    pos_data.append((tokens2id[p[0]], tokens2id[p[1]]))\n",
    "    \n",
    "for n in neg_context_samples:\n",
    "    \n",
    "    word_id = tokens2id[n[0]]\n",
    "    neg_samples = n[1]\n",
    "    \n",
    "    neg_ids = []\n",
    "    \n",
    "    for ns in neg_samples:\n",
    "        neg_ids.append(tokens2id[ns])\n",
    "        \n",
    "    neg_data.append((word_id, neg_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17127853 17127853 17127853 17127853\n"
     ]
    }
   ],
   "source": [
    "print(len(pos_data), len(neg_data), len(pos_context_pairs), len(neg_context_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(747, 1469) ('division', '68') ('division', ['bring', 'decide', 'information', 'witnesses', 'Nisga']) (747, [1747, 2938, 1613, 2526, 345])\n"
     ]
    }
   ],
   "source": [
    "print(pos_data[0], pos_context_pairs[0], neg_context_samples[0], neg_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(pos_data, open('pos_data' + label + '.p', 'wb'))\n",
    "pickle.dump(neg_data, open('neg_data' + label + '.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('pos_data' + label + '.p', 'rb') as f:\n",
    "    pos_data = pickle.load(f)\n",
    "    \n",
    "with open('neg_data' + label + '.p', 'rb') as f:\n",
    "    neg_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3276\n"
     ]
    }
   ],
   "source": [
    "with open('unigram_probs' + label +'.p', 'rb') as f:\n",
    "    unigram_probs = pickle.load(f)\n",
    "\n",
    "tokens = list(unigram_probs.keys())\n",
    "#default dictionary key:id value:token\n",
    "id2tokens = defaultdict(str)\n",
    "                        \n",
    "for i in range(len(tokens)):\n",
    "    id2tokens[i] = tokens[i]\n",
    "    \n",
    "#default dictionary key:token value:id\n",
    "tokens2id = defaultdict(int)\n",
    "\n",
    "for ind in id2tokens:\n",
    "    tokens2id[id2tokens[ind]] = ind\n",
    "    \n",
    "vocabulary_size = len(tokens2id)\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('tokens2id' +label+'.pickle', 'wb') as f:\n",
    "    pickle.dump(tokens2id,f)\n",
    "with open('id2tokens' +label+'.pickle', 'wb') as f:\n",
    "    pickle.dump(id2tokens,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
