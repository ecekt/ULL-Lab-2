{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import spatial\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "from random import randint\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_count = 2\n",
    "embedding_dim = 100\n",
    "det_embedding_dim = 128\n",
    "batch_size = 50\n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "window_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('corpus2id_hansard_en.pickle', 'rb') as f:\n",
    "    corpus2id_en = pickle.load(f)\n",
    "    \n",
    "with open('corpus2id_hansard_fr.pickle', 'rb') as f:\n",
    "    corpus2id_fr = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(corpus2id_en), len(corpus2id_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unigram_probs_hansard_en.p', 'rb') as f:\n",
    "    unigram_en = pickle.load(f)\n",
    "    \n",
    "with open('unigram_probs_hansard_fr.p', 'rb') as f:\n",
    "    unigram_fr = pickle.load(f)\n",
    "    \n",
    "vocabulary_size_en = len(unigram_en)\n",
    "vocabulary_size_fr = len(unigram_fr)\n",
    "print(vocabulary_size_en, vocabulary_size_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_s_len_en = 0\n",
    "max_s_len_fr = 0\n",
    "\n",
    "\n",
    "for s in range(len(corpus2id_en)):\n",
    "    \n",
    "    if len(corpus2id_en[s]) > max_s_len_en:\n",
    "        max_s_len_en = len(corpus2id_en[s])\n",
    "        \n",
    "    if len(corpus2id_fr[s]) > max_s_len_fr:\n",
    "        max_s_len_fr = len(corpus2id_fr[s])\n",
    "        \n",
    "print('Maximum sentence length EN:', max_s_len_en)\n",
    "print('Maximum sentence length FR:', max_s_len_fr)\n",
    "\n",
    "# HANSARDS\n",
    "# Maximum sentence length EN: 266\n",
    "# Maximum sentence length FR: 352"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_batches_EA(corpus_en, vocabulary_size_en, corpus_fr, vocabulary_size_fr, max_sentence_length_en,max_sentence_length_fr, batch_size):\n",
    "    \n",
    "    batches_en = []\n",
    "    batches_fr = []\n",
    "    \n",
    "    batch_number = len(corpus_en) // batch_size\n",
    "    print(batch_number)\n",
    "    no_sentences = len(corpus_en)\n",
    "    \n",
    "    indices = np.arange(0, no_sentences)\n",
    "        \n",
    "    #shuffle set\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for bn in range(batch_number):\n",
    "        \n",
    "        b_indices = indices[bn*batch_size:bn*batch_size + batch_size]\n",
    "        \n",
    "        batch_max_en = 0\n",
    "        batch_max_fr= 0\n",
    "        \n",
    "        for d in b_indices:\n",
    "            sent_len = len(corpus_en[d])\n",
    "            \n",
    "            if sent_len > batch_max_en:\n",
    "                batch_max_en = sent_len\n",
    "                \n",
    "            sent_len = len(corpus_fr[d])\n",
    "            \n",
    "            if sent_len > batch_max_fr:\n",
    "                batch_max_fr = sent_len\n",
    "                \n",
    "        sentence_b_en = []\n",
    "        sentence_b_fr = []\n",
    "        \n",
    "        for d in b_indices:\n",
    "            \n",
    "            sent_en = corpus_en[d]\n",
    "            \n",
    "            dif = batch_max_en - len(sent_en)\n",
    "            \n",
    "            for wd in range(dif):\n",
    "                sent_en.append(vocabulary_size_en)\n",
    "                \n",
    "            sent_fr = corpus_fr[d]\n",
    "            \n",
    "            dif = batch_max_fr - len(sent_fr)\n",
    "            \n",
    "            for wd in range(dif):\n",
    "                sent_fr.append(vocabulary_size_fr)\n",
    "                              \n",
    "                    \n",
    "            sentence_b_en.append(sent_en)\n",
    "        \n",
    "            sentence_b_en.append(sent_fr)\n",
    "    \n",
    "            \n",
    "        batches_en.append(sent_en)\n",
    "        \n",
    "        batches_fr.append(sent_fr)\n",
    "    \n",
    "    return batches_en, batches_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_en, batches_fr = create_batches_EA(corpus2id_en, vocabulary_size_en, corpus2id_fr, vocabulary_size_fr, max_s_len_en,max_s_len_fr, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EmbedAlign(nn.Module):\n",
    "    def __init__(self, vocab_size_en,vocab_size_fr, embedding_dim):\n",
    "        super(EmbedAlign, self).__init__()\n",
    "        \n",
    "        #for the inference model\n",
    "        self.w_embeddings = nn.Embedding(vocab_size_en, embedding_dim)\n",
    "        #encoder        \n",
    "        self.bidirLSTM = nn.LSTM(embedding_dim, embedding_dim, bidirectional=True)\n",
    "        #h_i = hi< + hi>\n",
    "        \n",
    "        self.mu_1 = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.mu_2 = nn.Linear(embedding_dim, embedding_dim)\n",
    "            \n",
    "        self.sigma_1 = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.sigma_2 = nn.Linear(embedding_dim, embedding_dim)\n",
    "        \n",
    "        #for the generative model\n",
    "        self.affine1L1 = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.affine2L1 = nn.Linear(embedding_dim, vocab_size_en)\n",
    "        \n",
    "        self.affine1L2 = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.affine2L2 = nn.Linear(embedding_dim, vocab_size_fr)\n",
    "       \n",
    "      \n",
    "    def forward(self, batch_en, batch_fr):\n",
    "        embeddings = self.w_embeddings(batch_en)\n",
    "        \n",
    "        output, hidden = self.bidirLSTM(embeddings) #  \n",
    "        hid_f = hidden[0][0]\n",
    "        hid_b = hidden[0][1]\n",
    "        \n",
    "        output = hid_f + hid_b\n",
    "        \n",
    "        mu = self.mu_1(output.squeeze())\n",
    "        mu = F.relu(mu)\n",
    "        mu = self.mu_2(mu)\n",
    "        \n",
    "        sigma = self.sigma_1(output.squeeze())\n",
    "        sigma = F.relu(sigma)\n",
    "        sigma = self.sigma_2(sigma)\n",
    "        sigma = F.softplus(sigma)\n",
    "        \n",
    "        epsilon = torch.distributions.multivariate_normal.MultivariateNormal(torch.zeros(dims),torch.diag(torch.ones(dims))).sample()\n",
    "        \n",
    "        #reparameterize\n",
    "        zi = mu + epsilon * sigma\n",
    "        \n",
    "        \n",
    "        #generative using sampled zi\n",
    "        #same zi for x and y\n",
    "        \n",
    "        xi = self.affine1L1(zi)\n",
    "        xi = F.relu(xi)\n",
    "        xi = self.affine2L1(xi)\n",
    "        xi = F.log_softmax(xi) #cat generation - target\n",
    "        \n",
    "        yi = self.affine1L1(zi)\n",
    "        yi = F.relu(yi)\n",
    "        yi = self.affine2L1(yi)\n",
    "        yi = F.log_softmax(yi) #cat generation - source\n",
    "        \n",
    "        alignment_dist = 1 / len(batch_en) #m being the sentence length of L1\n",
    "        \n",
    "    \n",
    "#         logx = \n",
    "#         logy = \n",
    "#         KL = -\n",
    "        \n",
    "        final_out = logx + logy + KL\n",
    "        return final_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 150\n",
    "model = EmbedAlign(vocabulary_size_en, vocabulary_size_fr, embedding_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "losses = []\n",
    "avg_losses = []\n",
    "\n",
    "print('epoch, total loss, average loss, duration')\n",
    "for e in range(epochs):\n",
    "    \n",
    "    then = datetime.now()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    for b in range(len(batches_en)):\n",
    "        \n",
    "        batch_en = torch.Tensor(np.asarray(batches_en[b])).long()\n",
    "           \n",
    "        print(batch_en)\n",
    "        batch_fr = torch.Tensor(np.asarray(batches_fr[b])).long()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = model(batches_en, batches_fr)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()       \n",
    "    \n",
    "    now = datetime.now()\n",
    "        \n",
    "    losses.append(total_loss)\n",
    "    \n",
    "    avg_loss = np.mean(losses)/no_batch\n",
    "    \n",
    "    print(e, total_loss, avg_loss, now-then)\n",
    "    \n",
    "    avg_losses.append(avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_one_hot(index, vocab_size):\n",
    "    \n",
    "    one_hot = np.zeros(vocab_size)\n",
    "    one_hot[index] += 1\n",
    "    \n",
    "    one_hot = torch.from_numpy(one_hot)\n",
    "        \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convert_to_one_hot(2,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sign(torch.tensor([i for i in range(9)], dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
