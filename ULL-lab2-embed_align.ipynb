{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import spatial\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions.kl as kl\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "from random import randint\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_count = 5\n",
    "embedding_dim = 100\n",
    "det_embedding_dim = 128\n",
    "batch_size = 100\n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "window_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read a subset 10000 sentences\n",
    "with open('corpus2id_hansard_en_subs.pickle', 'rb') as f:\n",
    "    corpus2id_en = pickle.load(f)\n",
    "    \n",
    "with open('corpus2id_hansard_fr_subs.pickle', 'rb') as f:\n",
    "    corpus2id_fr = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 10000\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus2id_en), len(corpus2id_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6330 7199\n"
     ]
    }
   ],
   "source": [
    "with open('unigram_probs_hansard_en_subs.p', 'rb') as f:\n",
    "    unigram_en = pickle.load(f)\n",
    "    \n",
    "with open('unigram_probs_hansard_fr_subs.p', 'rb') as f:\n",
    "    unigram_fr = pickle.load(f)\n",
    "    \n",
    "vocabulary_size_en = len(unigram_en)\n",
    "vocabulary_size_fr = len(unigram_fr)\n",
    "print(vocabulary_size_en, vocabulary_size_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sentence length EN: 185\n",
      "Maximum sentence length FR: 275\n"
     ]
    }
   ],
   "source": [
    "max_s_len_en = 0\n",
    "max_s_len_fr = 0\n",
    "\n",
    "\n",
    "for s in range(len(corpus2id_en)):\n",
    "    \n",
    "    if len(corpus2id_en[s]) > max_s_len_en:\n",
    "        max_s_len_en = len(corpus2id_en[s])\n",
    "        \n",
    "    if len(corpus2id_fr[s]) > max_s_len_fr:\n",
    "        max_s_len_fr = len(corpus2id_fr[s])\n",
    "        \n",
    "print('Maximum sentence length EN:', max_s_len_en)\n",
    "print('Maximum sentence length FR:', max_s_len_fr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_batches_EA(corpus_en, vocabulary_size_en, corpus_fr, vocabulary_size_fr, max_sentence_length_en,max_sentence_length_fr, batch_size):\n",
    "    \n",
    "    print(max_sentence_length_en, max_sentence_length_fr)\n",
    "    batches_en = []\n",
    "    batches_fr = []\n",
    "    \n",
    "    no_sentences = len(corpus_en)\n",
    "    \n",
    "    indices = np.arange(0, no_sentences)\n",
    "        \n",
    "    #shuffle set\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for bn in range(no_sentences):\n",
    "        \n",
    "        b_indices = indices[bn*batch_size:bn*batch_size + batch_size]\n",
    "        \n",
    "        batch_max_en = 0\n",
    "        batch_max_fr= 0\n",
    "        \n",
    "        for d in b_indices:\n",
    "            sent_len = len(corpus_en[d])\n",
    "            \n",
    "            if sent_len > batch_max_en:\n",
    "                batch_max_en = sent_len\n",
    "                \n",
    "            sent_len = len(corpus_fr[d])\n",
    "            \n",
    "            if sent_len > batch_max_fr:\n",
    "                batch_max_fr = sent_len\n",
    "                \n",
    "        sentence_b_en = []\n",
    "        sentence_b_fr = []\n",
    "        \n",
    "        for d in b_indices:\n",
    "            \n",
    "            sent_en = corpus_en[d]\n",
    "            \n",
    "#             dif = batch_max_en - len(sent_en)\n",
    "            \n",
    "#             for wd in range(dif):\n",
    "#                 sent_en.append(vocabulary_size_en)\n",
    "                \n",
    "            sent_fr = corpus_fr[d]\n",
    "            \n",
    "#             dif = batch_max_fr - len(sent_fr)\n",
    "            \n",
    "#             for wd in range(dif):\n",
    "#                 sent_fr.append(vocabulary_size_fr)\n",
    "                \n",
    "#             sentence_b_en.append(sent_en)\n",
    "        \n",
    "#             sentence_b_fr.append(sent_fr)\n",
    "    \n",
    "            \n",
    "        batches_en.append(sent_en)\n",
    "        \n",
    "        batches_fr.append(sent_fr)\n",
    "    \n",
    "    return batches_en, batches_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185 275\n"
     ]
    }
   ],
   "source": [
    "batches_en, batches_fr = create_batches_EA(corpus2id_en, vocabulary_size_en, corpus2id_fr, vocabulary_size_fr, max_s_len_en,max_s_len_fr, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2064, 434, 3741, 887, 3977, 4157, 1198, 3152, 4162, 4349]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches_en[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch, total loss, average loss, duration\n",
      "st 0\n",
      "st 1000\n",
      "st 2000\n",
      "st 3000\n",
      "st 4000\n",
      "st 5000\n",
      "st 6000\n",
      "st 7000\n",
      "st 8000\n"
     ]
    }
   ],
   "source": [
    "class EmbedAlign(nn.Module):\n",
    "    def __init__(self, vocab_size_en,vocab_size_fr, embedding_dim):\n",
    "        super(EmbedAlign, self).__init__()\n",
    "        \n",
    "        self.vocab_size_en = vocab_size_en\n",
    "        self.vocab_size_fr = vocab_size_fr\n",
    "        \n",
    "        #for the inference model\n",
    "        self.w_embeddings = nn.Embedding(vocab_size_en, embedding_dim)\n",
    "        #encoder        \n",
    "        self.bidirLSTM = nn.LSTM(embedding_dim, embedding_dim, bidirectional=True)\n",
    "        #h_i = hi< + hi>\n",
    "        \n",
    "        self.mu_1 = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.mu_2 = nn.Linear(embedding_dim, embedding_dim)\n",
    "            \n",
    "        self.sigma_1 = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.sigma_2 = nn.Linear(embedding_dim, embedding_dim)\n",
    "        \n",
    "        #for the generative model\n",
    "        self.affine1L1 = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.affine2L1 = nn.Linear(embedding_dim, vocab_size_en)\n",
    "        \n",
    "        self.affine1L2 = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.affine2L2 = nn.Linear(embedding_dim, vocab_size_fr)\n",
    "       \n",
    "        self.dist_norm = torch.distributions.multivariate_normal.MultivariateNormal(torch.ones(embedding_dim),torch.diag(torch.ones(embedding_dim)))\n",
    "            \n",
    "    def forward(self, batch_en, batch_fr, mu_i, sigma_i, z_i):\n",
    "        \n",
    "        kl_score = 0.0\n",
    "        sent_logx = 0.0\n",
    "        sent_logy = 0.0\n",
    "        \n",
    "        m = len(batch_en)\n",
    "        \n",
    "        for x in batch_en:\n",
    "            embeddings = self.w_embeddings(x)\n",
    "            #view_shape = embeddings.shape[0]\n",
    "            output, (hidden, cell) = self.bidirLSTM(embeddings.view(1, 1, -1)) \n",
    "\n",
    "            hid_f = hidden[0]\n",
    "            hid_b = hidden[1]\n",
    "\n",
    "            conc_hids = hid_f + hid_b\n",
    "            \n",
    "            mu = self.mu_1(conc_hids.squeeze())\n",
    "            mu = F.relu(mu)\n",
    "            mu = self.mu_2(mu)\n",
    "\n",
    "            sigma = self.sigma_1(conc_hids.squeeze())\n",
    "            sigma = F.relu(sigma)\n",
    "            sigma = self.sigma_2(sigma)\n",
    "            sigma = F.softplus(sigma)\n",
    "\n",
    "            mu_i[x] = mu\n",
    "            sigma_i[x] = sigma\n",
    "\n",
    "            epsilon = torch.distributions.multivariate_normal.MultivariateNormal(torch.zeros(embedding_dim),torch.diag(torch.ones(embedding_dim))).sample()\n",
    "\n",
    "            #reparameterize\n",
    "            z_i[x] = mu + epsilon * sigma\n",
    "\n",
    "            #generative using sampled zi\n",
    "            #variational location and scale\n",
    "            #same zi for x and y\n",
    "\n",
    "            zi = z_i[x] #sampled z\n",
    "\n",
    "            xi = self.affine1L1(zi)\n",
    "            xi = F.relu(xi)\n",
    "            xi = self.affine2L1(xi)\n",
    "            xi = F.log_softmax(xi, dim=0) #cat generation - target\n",
    "\n",
    "            yi = self.affine1L2(zi)\n",
    "            yi = F.relu(yi)\n",
    "            yi = self.affine2L2(yi)\n",
    "            yi = F.log_softmax(yi, dim=0) #cat generation - source\n",
    "\n",
    "            sent_logx += xi[x]\n",
    "            \n",
    "            #mu[torch.isnan(mu)] = 0\n",
    "            #print(mu_i[x], sigma_i[x])\n",
    "            \n",
    "            kl_loss = -(1 + torch.log(sigma**2) - mu ** 2 - sigma**2)/2\n",
    "            \n",
    "            kl_score += kl_loss\n",
    "            \n",
    "            best_j = 0\n",
    "            best_prob = 0\n",
    "            for y in batch_fr:\n",
    "                y_prob = yi[y].data\n",
    "                if y_prob > best_prob:\n",
    "                    best_prob = y_prob\n",
    "                    best_j = y #TODO\n",
    "                    \n",
    "            sent_logy += (-torch.Tensor([np.log(m)])) + best_prob\n",
    "                \n",
    "        final_out = -sent_logx - sent_logy + torch.sum(kl_score)\n",
    "            \n",
    "        return final_out, mu_i, sigma_i, xi,yi,z_i\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "model = EmbedAlign(vocabulary_size_en, vocabulary_size_fr, embedding_dim) #pad\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "losses = []\n",
    "avg_losses = []\n",
    "\n",
    "z_i = defaultdict()\n",
    "mu_i = defaultdict()\n",
    "sigma_i = defaultdict()\n",
    "        \n",
    "portion = 10000\n",
    "\n",
    "print('epoch, total loss, average loss, duration')\n",
    "for e in range(epochs):\n",
    "    \n",
    "    then = datetime.now()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    for b in range(portion):\n",
    "        \n",
    "        if len(batches_en[b]) > 0 and len(batches_fr[b]) > 0:\n",
    "            batch_en = torch.tensor(np.asarray(batches_en[b]), dtype= torch.long)\n",
    "\n",
    "            batch_fr = torch.tensor(np.asarray(batches_fr[b]), dtype= torch.long)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss, mu_i, sigma_i, xi,yi,z_i = model(batch_en, batch_fr,mu_i, sigma_i, z_i)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()       \n",
    "            \n",
    "            if b % 1000 == 0:\n",
    "                print('st', b)\n",
    "    \n",
    "    now = datetime.now()\n",
    "        \n",
    "    losses.append(total_loss)\n",
    "    \n",
    "    avg_loss = np.mean(losses)/len(batches_en)\n",
    "    \n",
    "    print(e, total_loss, avg_loss, now-then)\n",
    "    \n",
    "    avg_losses.append(avg_loss)\n",
    "    \n",
    "with open('mu_' + str(e) + '.pickle', 'wb') as file:\n",
    "    pickle.dump(mu_i, file)\n",
    "with open('sigma_' + str(e) + '.pickle', 'wb') as file:\n",
    "    pickle.dump(sigma_i, file)\n",
    "with open('xi_' + str(e) + '.pickle', 'wb') as file:\n",
    "    pickle.dump(xi, file)\n",
    "with open('yi_' + str(e) + '.pickle', 'wb') as file:\n",
    "    pickle.dump(yi, file)\n",
    "\n",
    "iteration= list(range(len(losses)))\n",
    "\n",
    "plt.plot(iteration, losses)\n",
    "plt.xlabel(\"Iterations for Embed-Align\")\n",
    "plt.ylabel('Average loss')\n",
    "plt.title('Evolution of the loss as a function of the iteration')\n",
    "plt.savefig(\"embed.png\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "dist1 = torch.distributions.multivariate_normal.MultivariateNormal(torch.ones(embedding_dim),torch.diag(torch.ones(embedding_dim)))\n",
    "dist2 = torch.distributions.multivariate_normal.MultivariateNormal(torch.ones(embedding_dim),torch.diag(torch.ones(embedding_dim)))\n",
    "\n",
    "kl_score = kl.kl_divergence(dist1,dist2)\n",
    "print(kl_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_kl_divergence(m1, m2, s1, s2, embedding_dim):\n",
    "    \n",
    "    epsilon = torch.distributions.multivariate_normal.MultivariateNormal(torch.zeros(embedding_dim),torch.diag(torch.ones(embedding_dim))).sample()\n",
    "\n",
    "    kl_score = kl.kl_divergence()\n",
    "    return kl_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iteration= list(range(len(avg_losses)))\n",
    "\n",
    "plt.plot(iteration, avg_losses)\n",
    "plt.xlabel(\"Iterations for Embed-Align\")\n",
    "plt.ylabel('Average loss')\n",
    "plt.title('Evolution of average loss as a function of the iteration')\n",
    "plt.savefig(\"embedalign.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_one_hot(index, vocab_size):\n",
    "    \n",
    "    one_hot = np.zeros(vocab_size)\n",
    "    one_hot[index] += 1\n",
    "    \n",
    "    one_hot = torch.from_numpy(one_hot)\n",
    "        \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convert_to_one_hot(2,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.sign(torch.tensor([i for i in range(9)], dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
