{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import spatial\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "from random import randint\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('error')\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import spatial\n",
    "\n",
    "import string\n",
    "puncs = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = '_hansard_en' \n",
    "#label = '_hansards_fr'\n",
    "\n",
    "with open('tokens2id' +label +'_smaller'+'.pickle', 'rb') as f:\n",
    "    tokens2id = pickle.load(f)\n",
    "with open('id2tokens' +label+'_smaller'+'.pickle', 'rb') as f:\n",
    "    id2tokens = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2949"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wordvecs_skipgram_word' + label + '_smaller' + str(100) + '.pickle', 'rb') as file:\n",
    "    w_embeds = pickle.load(file)\n",
    "\n",
    "with open('wordvecs_skipgram_context' + label +  '_smaller' + str(100) + '.pickle', 'rb') as file:\n",
    "    c_embeds = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_embeds[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('lst/lst.gold.candidates') as file:\n",
    "    whole_gold = file.read().splitlines() \n",
    "    \n",
    "    gold_cands_l = [l.split('::') for l in whole_gold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gold_cands = defaultdict(dict)\n",
    "\n",
    "for g in gold_cands_l:\n",
    "    word_pos = g[0]\n",
    "    \n",
    "    word, postag = word_pos.split('.')\n",
    "    \n",
    "    candidates = g[1].split(';')\n",
    "    \n",
    "    gold_cands[word] = {'postag':postag, 'candidates':candidates}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'candidates': ['alight',\n",
       "  'skilled',\n",
       "  'deep',\n",
       "  'good',\n",
       "  'sharp',\n",
       "  'luminous',\n",
       "  'colourful',\n",
       "  'optimisitc',\n",
       "  'vivid',\n",
       "  'capable',\n",
       "  'positive',\n",
       "  'hopeful',\n",
       "  'shining',\n",
       "  'intelligent',\n",
       "  'smart',\n",
       "  'clever',\n",
       "  'motivated',\n",
       "  'vibrant',\n",
       "  'up-and-coming',\n",
       "  'well-lit',\n",
       "  'gleam',\n",
       "  'most talented',\n",
       "  'great',\n",
       "  'talented',\n",
       "  'most able',\n",
       "  'brilliant',\n",
       "  'light',\n",
       "  'clear',\n",
       "  'gifted',\n",
       "  'promising'],\n",
       " 'postag': 'a'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_cands['bright']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('lst/lst_test.preprocessed') as file:\n",
    "    \n",
    "    lst_sentences = [l.split() for l in file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lst_test_preprocessed = defaultdict(dict)\n",
    "\n",
    "for l in lst_sentences:\n",
    "    word, postag = l[0].split('.')\n",
    "    sentence_no = int(l[1])\n",
    "    word_position = int(l[2])\n",
    "    sentence_tokens = l[3:]\n",
    "    \n",
    "    processed_tokens = []\n",
    "    \n",
    "    #remove punctuations TODO position might change\n",
    "    punc_inds = []\n",
    "    for s in range(len(sentence_tokens)):\n",
    "        \n",
    "        if sentence_tokens[s] not in puncs:\n",
    "            processed_tokens.append(sentence_tokens[s])\n",
    "        else:\n",
    "            punc_inds.append(s)\n",
    "                \n",
    "    temp_word_position = word_position\n",
    "    \n",
    "    for d in punc_inds:\n",
    "        if d < temp_word_position:\n",
    "            word_position -= 1\n",
    "        \n",
    "    lst_test_preprocessed[(word,sentence_no)] = {'postag':postag, 'word_position':word_position,'sentence':processed_tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'postag': 'n',\n",
       " 'sentence': ['some',\n",
       "  'darting',\n",
       "  'terms',\n",
       "  'originated',\n",
       "  'outside',\n",
       "  'the',\n",
       "  'sport',\n",
       "  'such',\n",
       "  'as',\n",
       "  'hat',\n",
       "  'trick',\n",
       "  'meaning',\n",
       "  'all',\n",
       "  'three',\n",
       "  'darts',\n",
       "  'of',\n",
       "  'a',\n",
       "  'player',\n",
       "  \"'s\",\n",
       "  'round',\n",
       "  'landing',\n",
       "  'in',\n",
       "  'the',\n",
       "  'bull'],\n",
       " 'word_position': 23}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_test_preprocessed[('bull',589)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# base word embed\n",
    "#word+context combination\n",
    "#levy lexical subs paper has the formulas\n",
    "\n",
    "#add\n",
    "# mul\n",
    "# baladd\n",
    "# balmul\n",
    "# kl\n",
    "#gold standard is lemmatized\n",
    "#noun verb adverb adj\n",
    "#skipgram context insensitive but we can make it sensitive via using context embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SkipGramNeg(nn.Module):\n",
    "    def __init__(self, vocabulary_size, embedding_dim):\n",
    "        super(SkipGramNeg, self).__init__()\n",
    "        \n",
    "        #sparse embeddings for word and context vectors\n",
    "        \n",
    "        self.w_embeddings = nn.Embedding(vocabulary_size, embedding_dim, sparse = True)\n",
    "        self.c_embeddings = nn.Embedding(vocabulary_size, embedding_dim, sparse = True)\n",
    "        \n",
    "        # initialization of embeds\n",
    "        # https://adoni.github.io/2017/11/08/word2vec-pytorch/\n",
    "\n",
    "        initrange = 0.5 / embedding_dim\n",
    "        self.w_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.c_embeddings.weight.data.uniform_(-0, 0)\n",
    "\n",
    "    def forward(self, pos_words, pos_conts, neg_conts):\n",
    "        \n",
    "        #Loss calculation, Levy&Goldberg word2vec Explained\n",
    "        #https://adoni.github.io/2017/11/08/word2vec-pytorch/\n",
    "        \n",
    "        w_out = self.w_embeddings(pos_words)\n",
    "        \n",
    "        pos_out = self.c_embeddings(pos_conts)\n",
    "        neg_out = self.c_embeddings(neg_conts)\n",
    "        \n",
    "#         print(neg_conts)\n",
    "#         print(pos_conts)\n",
    "#         print(pos_words)\n",
    "               \n",
    "        pos_val = torch.mul(w_out, pos_out).squeeze()\n",
    "        pos_val = torch.sum(pos_val, dim = 1)\n",
    "        pos_loss = F.logsigmoid(pos_val)\n",
    "        \n",
    "        neg_val = torch.bmm(neg_out, w_out.unsqueeze(2)).squeeze()\n",
    "        neg_val = torch.sum(neg_val, dim = 1)\n",
    "        neg_loss = F.logsigmoid(-neg_val)\n",
    "        \n",
    "        final_out = pos_loss + neg_loss.sum()\n",
    "        final_out = -final_out.sum()/len(pos_words) #neg and mean\n",
    "         \n",
    "        return final_out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_context_window(sentence, central_word_index, window_size):\n",
    "    \n",
    "    context = []\n",
    "    word = sentence[central_word_index]\n",
    "    \n",
    "    for w in range(1,window_size+1):\n",
    "        \n",
    "        left_cont = central_word_index - w\n",
    "        right_cont = central_word_index + w\n",
    "        \n",
    "        #find the window words to the left and right\n",
    "        #add as pair if they are inside sentence boundaries\n",
    "        \n",
    "        if left_cont > -1:\n",
    "            context.append(sentence[left_cont])\n",
    "            \n",
    "        if right_cont < len(sentence):\n",
    "            context.append(sentence[right_cont])\n",
    "            \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def out_ranked_results(word, postag, sentence_id, sorted_d):\n",
    "    \n",
    "    word_pos = str(word+'.'+postag)\n",
    "    res = 'RANKED\\t' + word_pos + ' '+ str(sentence_id)\n",
    "    \n",
    "    for d in sorted_d:\n",
    "        res += '\\t' + d[0]+' '+ str(d[1])\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lst_test_nouns = defaultdict(dict)\n",
    "# lst_test_verbs = defaultdict(dict)\n",
    "# lst_test_adjectives = defaultdict(dict)\n",
    "\n",
    "# with open('tokens2id.pickle') as f:\n",
    "#     tokens2id = pickle.load(f)\n",
    "# with open('id2tokens.pickle') as f:\n",
    "#     id2tokens = pickle.load(f)\n",
    "    \n",
    "window_size = 5\n",
    "\n",
    "# #embedding parameters\n",
    "# weights = model.w_embeddings.weight.detach()\n",
    "\n",
    "results_to_write = []\n",
    "\n",
    "for ls in lst_test_preprocessed:\n",
    "    \n",
    "    #GET WORD AND SENTENCE RELATED INFO\n",
    "    item = lst_test_preprocessed[ls]\n",
    "    central_word = ls[0]\n",
    "    sentence_id = ls[1]\n",
    "    \n",
    "    postag = item['postag']\n",
    "    word_position = item['word_position']\n",
    "    sentence = item['sentence']\n",
    "    \n",
    "    #get the list of candidate gold annotations\n",
    "    cands = gold_cands[central_word]['candidates']\n",
    "\n",
    "    cos_sims = defaultdict(float)\n",
    "    add_heur = defaultdict(float)\n",
    "    mul_heur = defaultdict(float)\n",
    "\n",
    "    if central_word in tokens2id:\n",
    "        id_token = tokens2id[central_word]\n",
    "\n",
    "        #get the embedding of the word\n",
    "        embed1 = w_embeds[tokens2id[central_word]]\n",
    "        \n",
    "        # GET CONTEXT WORDS HERE\n",
    "        context_words = get_context_window(sentence, word_position, window_size)\n",
    "\n",
    "        context_ids = []\n",
    "        context_vectors = []\n",
    "\n",
    "        for cw in context_words:\n",
    "            ctx_id = tokens2id[cw]\n",
    "            if ctx_id in c_embeds:\n",
    "                context_ids.append(ctx_id)\n",
    "                context_vectors.append(c_embeds[ctx_id])\n",
    "\n",
    "        for c in cands:\n",
    "\n",
    "            #for each candidate find the cosine similarity between target and central\n",
    "\n",
    "            id_for_cand = tokens2id[c]\n",
    "\n",
    "            if id_for_cand < len(w_embeds):\n",
    "                embed2 = w_embeds[id_for_cand]\n",
    "\n",
    "                cosine_similarity = 1 - spatial.distance.cosine(embed1, embed2)\n",
    "                cos_sims[c] = cosine_similarity\n",
    "            else:\n",
    "                cos_sims[c] = 0\n",
    "\n",
    "            #USING FORMULAS IN LEVY - LEXICAL SUBS.\n",
    "    #         add_res = \n",
    "    #         add_heur.append(add_res)\n",
    "\n",
    "    #         mul_res = \n",
    "    #         mul_heur.append(mul_res)\n",
    "\n",
    "        sorted_d = sorted(cos_sims.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        ranked_res = out_ranked_results(central_word, postag, sentence_id, sorted_d)\n",
    "\n",
    "        results_to_write.append(ranked_res)\n",
    "    else:\n",
    "        \n",
    "        for c in cands:\n",
    "\n",
    "            #for each candidate find the cosine similarity between target and central\n",
    "\n",
    "            id_for_cand = tokens2id[c]\n",
    "\n",
    "            cos_sims[c] = 0\n",
    "\n",
    "\n",
    "        sorted_d = sorted(cos_sims.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        ranked_res = out_ranked_results(central_word, postag, sentence_id, sorted_d)\n",
    "\n",
    "        results_to_write.append(ranked_res)\n",
    "\n",
    "file_name = 'skipgram' + label + '.txt'\n",
    "\n",
    "with open (file_name, 'w') as f:\n",
    " \n",
    "    for r in range(len(results_to_write)):\n",
    "\n",
    "        if r == len(results_to_write) - 1:\n",
    "            f.write(results_to_write[r])\n",
    "        else:\n",
    "            f.write(results_to_write[r])\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#multi word \n",
    "#words with no embeds\n",
    "\n",
    "#embed align eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'division'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2949"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
