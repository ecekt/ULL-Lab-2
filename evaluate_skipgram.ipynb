{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import spatial\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "from random import randint\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('error')\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import spatial\n",
    "\n",
    "import string\n",
    "puncs = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('wordvecs_skipgram.pickle', 'rb') as file:\n",
    "    w_embeds = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_embeds[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('lst/lst.gold.candidates') as file:\n",
    "    whole_gold = file.read().splitlines() \n",
    "    \n",
    "    gold_cands_l = [l.split('::') for l in whole_gold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gold_cands = defaultdict(dict)\n",
    "\n",
    "for g in gold_cands_l:\n",
    "    word_pos = g[0]\n",
    "    \n",
    "    word, postag = word_pos.split('.')\n",
    "    \n",
    "    candidates = g[1].split(';')\n",
    "    \n",
    "    gold_cands[word] = {'postag':postag, 'candidates':candidates}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'candidates': ['alight',\n",
       "  'skilled',\n",
       "  'deep',\n",
       "  'good',\n",
       "  'sharp',\n",
       "  'luminous',\n",
       "  'colourful',\n",
       "  'optimisitc',\n",
       "  'vivid',\n",
       "  'capable',\n",
       "  'positive',\n",
       "  'hopeful',\n",
       "  'shining',\n",
       "  'intelligent',\n",
       "  'smart',\n",
       "  'clever',\n",
       "  'motivated',\n",
       "  'vibrant',\n",
       "  'up-and-coming',\n",
       "  'well-lit',\n",
       "  'gleam',\n",
       "  'most talented',\n",
       "  'great',\n",
       "  'talented',\n",
       "  'most able',\n",
       "  'brilliant',\n",
       "  'light',\n",
       "  'clear',\n",
       "  'gifted',\n",
       "  'promising'],\n",
       " 'postag': 'a'}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_cands['bright']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('lst/lst_test.preprocessed') as file:\n",
    "    \n",
    "    lst_sentences = [l.split() for l in file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lst_test_preprocessed = defaultdict(dict)\n",
    "\n",
    "for l in lst_sentences:\n",
    "    word, postag = l[0].split('.')\n",
    "    sentence_no = int(l[1])\n",
    "    word_position = int(l[2])\n",
    "    sentence_tokens = l[3:]\n",
    "    \n",
    "    processed_tokens = []\n",
    "    \n",
    "    #remove punctuations TODO position might change\n",
    "    punc_inds = []\n",
    "    for s in range(len(sentence_tokens)):\n",
    "        \n",
    "        min_deleted_ind = len(sentence_tokens)\n",
    "        \n",
    "        if sentence_tokens[s] not in puncs:\n",
    "            processed_tokens.append(sentence_tokens[s])\n",
    "        else:\n",
    "            punc_inds.append(s)\n",
    "            \n",
    "            if s < min_deleted_ind:\n",
    "                min_deleted_ind = s\n",
    "    \n",
    "    for d in punc_inds:\n",
    "        if d < word_position:\n",
    "            word_position -= 1\n",
    "        \n",
    "    lst_test_preprocessed[(word,sentence_no)] = {'postag':postag, 'word_position':word_position,'sentence':processed_tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'postag': 'n',\n",
       " 'sentence': ['on',\n",
       "  'sunday',\n",
       "  'at',\n",
       "  'craven',\n",
       "  'cottage',\n",
       "  'jose',\n",
       "  'mourinho',\n",
       "  'and',\n",
       "  'his',\n",
       "  'all',\n",
       "  'stars',\n",
       "  'exhibited',\n",
       "  'all',\n",
       "  'of',\n",
       "  'the',\n",
       "  'above',\n",
       "  'symptoms',\n",
       "  'and',\n",
       "  'they',\n",
       "  'were',\n",
       "  'made',\n",
       "  'to',\n",
       "  'pay',\n",
       "  'the',\n",
       "  'price',\n",
       "  'by',\n",
       "  'a',\n",
       "  'fulham',\n",
       "  'side',\n",
       "  'that',\n",
       "  'had',\n",
       "  'in',\n",
       "  'previous',\n",
       "  'weeks',\n",
       "  'woken',\n",
       "  'up',\n",
       "  'after',\n",
       "  'matches',\n",
       "  'with',\n",
       "  'their',\n",
       "  'heads',\n",
       "  'kicked',\n",
       "  'in'],\n",
       " 'word_position': 28}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_test_preprocessed[('side',301)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# base word embed\n",
    "#word+context combination\n",
    "#levy lexical subs paper has the formulas\n",
    "\n",
    "#add\n",
    "# mul\n",
    "# baladd\n",
    "# balmul\n",
    "# kl\n",
    "#gold standard is lemmatized\n",
    "#noun verb adverb adj\n",
    "#skipgram context insensitive but we can make it sensitive via using context embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocabulary_size, embedding_dim):\n",
    "        super(SkipGram, self).__init__()\n",
    "        \n",
    "        #sparse embeddings for word and context vectors\n",
    "        self.w_embeddings = nn.Embedding(vocabulary_size, embedding_dim) #, sparse = True\n",
    "        self.lin1 = nn.Linear(embedding_dim, vocabulary_size, bias = False)\n",
    "           \n",
    "    def forward(self, pos_words):\n",
    "        \n",
    "        out = self.w_embeddings(pos_words)\n",
    "        \n",
    "        out = self.lin1(out)\n",
    "        \n",
    "        final_out = F.log_softmax(out, dim = 0)\n",
    "        \n",
    "        return final_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('skipgram.pickle', 'rb') as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SkipGramNeg(nn.Module):\n",
    "    def __init__(self, vocabulary_size, embedding_dim):\n",
    "        super(SkipGramNeg, self).__init__()\n",
    "        \n",
    "        #sparse embeddings for word and context vectors\n",
    "        \n",
    "        self.w_embeddings = nn.Embedding(vocabulary_size, embedding_dim, sparse = True)\n",
    "        self.c_embeddings = nn.Embedding(vocabulary_size, embedding_dim, sparse = True)\n",
    "        \n",
    "        # initialization of embeds\n",
    "        # https://adoni.github.io/2017/11/08/word2vec-pytorch/\n",
    "\n",
    "        initrange = 0.5 / embedding_dim\n",
    "        self.w_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.c_embeddings.weight.data.uniform_(-0, 0)\n",
    "\n",
    "    def forward(self, pos_words, pos_conts, neg_conts):\n",
    "        \n",
    "        #Loss calculation, Levy&Goldberg word2vec Explained\n",
    "        #https://adoni.github.io/2017/11/08/word2vec-pytorch/\n",
    "        \n",
    "        w_out = self.w_embeddings(pos_words)\n",
    "        \n",
    "        pos_out = self.c_embeddings(pos_conts)\n",
    "        neg_out = self.c_embeddings(neg_conts)\n",
    "        \n",
    "#         print(neg_conts)\n",
    "#         print(pos_conts)\n",
    "#         print(pos_words)\n",
    "               \n",
    "        pos_val = torch.mul(w_out, pos_out).squeeze()\n",
    "        pos_val = torch.sum(pos_val, dim = 1)\n",
    "        pos_loss = F.logsigmoid(pos_val)\n",
    "        \n",
    "        neg_val = torch.bmm(neg_out, w_out.unsqueeze(2)).squeeze()\n",
    "        neg_val = torch.sum(neg_val, dim = 1)\n",
    "        neg_loss = F.logsigmoid(-neg_val)\n",
    "        \n",
    "        final_out = pos_loss + neg_loss.sum()\n",
    "        final_out = -final_out.sum()/len(pos_words) #neg and mean\n",
    "         \n",
    "        return final_out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'skipgram_neg.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-ca25aff1a3da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skipgram_neg.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'skipgram_neg.pickle'"
     ]
    }
   ],
   "source": [
    "with open('skipgram_neg.pickle', 'rb') as file:\n",
    "    model_neg = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_context_window(sentence, central_word_index, window_size):\n",
    "    \n",
    "    context = []\n",
    "    word = sentence[central_word_index]\n",
    "    \n",
    "    for w in range(1,window_size+1):\n",
    "        \n",
    "        left_cont = central_word_index - w\n",
    "        right_cont = central_word_index + w\n",
    "        \n",
    "        #find the window words to the left and right\n",
    "        #add as pair if they are inside sentence boundaries\n",
    "        \n",
    "        if left_cont > -1:\n",
    "            context.append(sentence[left_cont])\n",
    "            \n",
    "        if right_cont < len(sentence):\n",
    "            context.append(sentence[right_cont])\n",
    "            \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens2id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-8be1cf4ff8fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontext_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mctx_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens2id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mcontext_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mcontext_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokens2id' is not defined"
     ]
    }
   ],
   "source": [
    "# lst_test_nouns = defaultdict(dict)\n",
    "# lst_test_verbs = defaultdict(dict)\n",
    "# lst_test_adjectives = defaultdict(dict)\n",
    "\n",
    "# with open('tokens2id.pickle') as f:\n",
    "#     tokens2id = pickle.load(f)\n",
    "# with open('id2tokens.pickle') as f:\n",
    "#     id2tokens = pickle.load(f)\n",
    "    \n",
    "window_size = 5\n",
    "\n",
    "#embedding parameters\n",
    "weights = model.w_embeddings.weight.detach()\n",
    "\n",
    "for ls in lst_test_preprocessed:\n",
    "    \n",
    "    #GET WORD AND SENTENCE RELATED INFO\n",
    "    item = lst_test_preprocessed[ls]\n",
    "    central_word = ls[0]\n",
    "    sentence_id = ls[1]\n",
    "    \n",
    "    postag = item['postag']\n",
    "    word_position = item['word_position']\n",
    "    sentence = item['sentence']\n",
    "    \n",
    "    # GET CONTEXT WORDS HERE\n",
    "    context_words = get_context_window(sentence, word_position, window_size)\n",
    "    \n",
    "    context_ids = []\n",
    "    context_vectors = []\n",
    "    \n",
    "    for cw in context_words:\n",
    "        ctx_id = tokens2id[cw]\n",
    "        context_ids.append(ctx_id)\n",
    "        context_vectors.append(weights(ctx_id))\n",
    "        \n",
    "    #get the embedding of the word\n",
    "    embed = model.w_embeddings.weight[tokens2id[central_word]]\n",
    "    \n",
    "    #get the list of candidate gold annotations\n",
    "    cands = gold_cands[central_word]['candidates']\n",
    "    \n",
    "    cos_sims = defaultdict(float)\n",
    "    add_heur = defaultdict(float)\n",
    "    mul_heur = defaultdict(float)\n",
    "    \n",
    "    for c in cands:\n",
    "        \n",
    "        #for each candidate find the cosine similarity between target and central\n",
    "        id_for_cand = tokens2id[c]\n",
    "        \n",
    "        embed2 = weights[id_for_cand]\n",
    "    \n",
    "        cosine_similarity = 1 - spatial.distance.cosine(embed1, embed2)\n",
    "        cos_sims[c] = cosine_similarity\n",
    "        \n",
    "        #USING FORMULAS IN LEVY - LEXICAL SUBS.\n",
    "#         add_res = \n",
    "#         add_heur.append(add_res)\n",
    "        \n",
    "#         mul_res = \n",
    "#         mul_heur.append(mul_res)\n",
    "\n",
    "    sorted_d = sorted(cos_sims.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
