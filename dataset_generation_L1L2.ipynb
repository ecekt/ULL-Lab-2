{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import spatial\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "from random import randint\n",
    "import pickle\n",
    "import nltk\n",
    "import string\n",
    "puncs = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "batch_size = 50\n",
    "min_count = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size 161769\n",
      "After rare word pruning 100797\n"
     ]
    }
   ],
   "source": [
    "# dataset = 'europarl/training.en'\n",
    "# label = '_europarl_en'\n",
    "\n",
    "dataset = 'europarl/training.fr'\n",
    "label = '_europarl_fr'\n",
    "\n",
    "# dataset = 'hansards/training.en'\n",
    "# label = '_hansard_en'\n",
    "\n",
    "# dataset = 'hansards/training.fr'\n",
    "# label = '_hansard_fr'\n",
    "\n",
    "# dataset = 'wa/dev.en'\n",
    "# label = '_dev'\n",
    "\n",
    "#read the files\n",
    "with open(dataset) as f:\n",
    "    sentences = [l.strip() for l in f.readlines()]\n",
    "\n",
    "#get all the tokens from the corpus\n",
    "\n",
    "tokens_list = []\n",
    "sentence_list = []\n",
    "for s in sentences:\n",
    "    split_sent = s.split()\n",
    "    sentence = []\n",
    "    for w in split_sent:\n",
    "        \n",
    "        tokens_list.append(w)\n",
    "        sentence.append(w)\n",
    "#         #filter stopwords\n",
    "#         if w not in stopwords:\n",
    "#             tokens_list.append(w)\n",
    "#             sentence.append(w)\n",
    "    \n",
    "    sentence_list.append(sentence)\n",
    "    \n",
    "tokens = list(sorted(set(tokens_list)))\n",
    "print('Vocabulary size', len(tokens))\n",
    "\n",
    "count_tokens = Counter(tokens_list)\n",
    "\n",
    "#words appearing fewer than this are not considered words or contexts\n",
    "#subsample frequent words\n",
    "\n",
    "temp_sentence_list = []\n",
    "\n",
    "for s in range(len(sentence_list)):\n",
    "    temp_sentence_list.append(sentence_list[s])\n",
    "\n",
    "#Find the infrequent words\n",
    "for s in range(len(temp_sentence_list)):\n",
    "    for w in range(len(temp_sentence_list[s])):\n",
    "        word = temp_sentence_list[s][w]\n",
    "        if count_tokens[word] < min_count:\n",
    "            sentence_list[s][w] = '<unk>'\n",
    "            \n",
    "            #remove the infrequent words from tokens\n",
    "            tokens.remove(word)\n",
    "\n",
    "#we prune rare words altogether\n",
    "\n",
    "#tokens.append('<unk>')\n",
    "vocab_size = len(tokens)\n",
    "print('After rare word pruning', vocab_size)\n",
    "#remove the infrequent words from the sentences\n",
    "for s in sentence_list:\n",
    "    s[:] = [w for w in s if w != '<unk>' and w not in puncs]\n",
    "\n",
    "flat_token_list = []\n",
    "\n",
    "for s in sentence_list:\n",
    "    for w in s:\n",
    "        flat_token_list.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_counts = Counter(flat_token_list)\n",
    "\n",
    "unigram_probs = defaultdict(float) \n",
    "\n",
    "for t in unigram_counts:\n",
    "    unigram_probs[t] = unigram_counts[t]/len(flat_token_list)\n",
    "\n",
    "pickle.dump(unigram_probs, open('unigram_probs' + label + '.p', 'wb'))\n",
    "\n",
    "unigram_probs = pickle.load(open('unigram_probs' + label + '.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100770\n"
     ]
    }
   ],
   "source": [
    "tokens = list(set(flat_token_list))\n",
    "\n",
    "#default dictionary key:id value:token\n",
    "id2tokens = defaultdict(str)\n",
    "                        \n",
    "for i in range(len(tokens)):\n",
    "    id2tokens[i] = tokens[i]\n",
    "    \n",
    "#default dictionary key:token value:id\n",
    "tokens2id = defaultdict(int)\n",
    "\n",
    "for ind in id2tokens:\n",
    "    tokens2id[id2tokens[ind]] = ind\n",
    "    \n",
    "vocabulary_size = len(tokens2id)\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dataset to ids\n",
    "corpus2id = []\n",
    "\n",
    "for s in sentence_list:\n",
    "    \n",
    "    sentence2id = []\n",
    "    \n",
    "    for w in s:\n",
    "        word_id = tokens2id[w]\n",
    "        sentence2id.append(word_id)\n",
    "        \n",
    "    corpus2id.append(sentence2id)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('corpus2id' + label + '.pickle', 'wb') as f:\n",
    "    pickle.dump(corpus2id, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('corpus2id' + label + '.pickle', 'rb') as f:\n",
    "    corpus2id = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24564, 21978]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus2id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
