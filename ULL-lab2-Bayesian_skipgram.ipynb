{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import spatial\n",
    "from scipy import stats\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from random import randint\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_data = pickle.load(open('pos_data.p', 'rb'))\n",
    "neg_data = pickle.load(open('neg_data.p', 'rb'))\n",
    "\n",
    "unigram_probs = pickle.load(open('unigram_probs.p', 'rb'))\n",
    "vocab_size = len(unigram_probs)\n",
    "\n",
    "central_words = []\n",
    "contexts = []\n",
    "neg_samples = []\n",
    "\n",
    "for p in pos_data:\n",
    "    central_words.append(p[0])\n",
    "    contexts.append(p[1])\n",
    "    \n",
    "for n in neg_data:\n",
    "    neg_samples.append(n[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 64 [44, 47, 19, 10, 79]\n"
     ]
    }
   ],
   "source": [
    "print(central_words[0], contexts[0], neg_samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type('torch.DoubleTensor')\n",
    "\n",
    "class BayesianSG(nn.Module):\n",
    "\n",
    "    def __init__(self,embedding_size, vocab_size):\n",
    "        super(BayesianSG, self).__init__()\n",
    "        self.w_embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.c_embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        \n",
    "        self.dense_weights = nn.Linear(vocab_size*2, embedding_size, bias = False)\n",
    "        \n",
    "        self.muLinear = nn.Linear(embedding_size, embedding_size)\n",
    "        self.sigmaLinear = nn.Linear(embedding_size, embedding_size)\n",
    "        self.fLinear = nn.Linear(embedding_size, vocab_size)\n",
    "        self.softm = nn.Softmax(dim=0)\n",
    "        \n",
    "        self.Location = nn.Linear(vocab_size,1, bias = False)\n",
    "        self.Scale = nn.Linear(vocab_size,1, bias = False)\n",
    "        \n",
    "        \n",
    "    def forward(self, central_words, contexts):\n",
    "        \n",
    "        out = torch.cat((central_words, contexts), 1) #concatenate dense vectors\n",
    "        \n",
    "        out = self.dense_weights(out)\n",
    "        \n",
    "        out = F.relu(out)\n",
    "        \n",
    "        out = torch.sum(out,0)\n",
    "        \n",
    "        mu_posterior = self.muLinear(out)\n",
    "        \n",
    "        sigma_posterior = self.sigmaLinear(out)\n",
    "        \n",
    "        sigma_posterior = F.softplus(sigma_posterior)\n",
    "        \n",
    "        dims = mu_posterior.shape\n",
    "        \n",
    "        epsilon = torch.distributions.multivariate_normal.MultivariateNormal \\\n",
    "        (torch.zeros(dims),torch.diag(torch.ones(dims)))\n",
    "        \n",
    "        z = mu_posterior + sigma_posterior * epsilon.sample()\n",
    "        \n",
    "        out = self.fLinear(z)\n",
    "        \n",
    "        probs = self.softm(out)\n",
    "        \n",
    "        mu_prior = self.Location(central_words)\n",
    "        \n",
    "        sigma_prior = self.Scale(central_words)\n",
    "        sigma_prior = F.softplus(sigma_prior)\n",
    "        \n",
    "        return mu_posterior, sigma_posterior, mu_prior, sigma_prior\n",
    "    \n",
    "embedding_size = 10 \n",
    "model = BayesianSG(embedding_size, vocab_size)\n",
    "\n",
    "#mu_posterior, sigma_posterior, mu_prior, sigma_prior = model(central_words, contexts, neg_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
