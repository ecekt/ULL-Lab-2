{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import spatial\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "from random import randint\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('pos_data.p', 'rb') as f:\n",
    "    pos_data = pickle.load(f)\n",
    "    \n",
    "with open('neg_data.p', 'rb') as f:\n",
    "    neg_data = pickle.load(f)\n",
    "\n",
    "with open('unigram_probs.p', 'rb') as f:\n",
    "    unigram_probs = pickle.load(f)\n",
    "    \n",
    "vocab_size = len(unigram_probs)\n",
    "\n",
    "central_words = []\n",
    "contexts = []\n",
    "neg_samples = []\n",
    "\n",
    "for p in pos_data:\n",
    "    central_words.append(p[0])\n",
    "    contexts.append(p[1])\n",
    "    \n",
    "for n in neg_data:\n",
    "    neg_samples.append(n[1])\n",
    "    \n",
    "dataset = [central_words, contexts, neg_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(dataset, batch_size):\n",
    "    \n",
    "    batch_number = len(dataset[0]) // batch_size\n",
    "    no_central_words = len(dataset[0])\n",
    "    pos_words = []\n",
    "    pos_contexts = []\n",
    "    neg_contexts = []\n",
    "    \n",
    "    for bn in range(batch_number):\n",
    "        indices = np.arange(0, no_central_words)\n",
    "        \n",
    "        #shuffle set\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        indices = indices[0:batch_size]\n",
    "        #shuffle dataset\n",
    "        \n",
    "        central = []\n",
    "        contx = []\n",
    "        negs = []\n",
    "        \n",
    "        for d in indices:\n",
    "            \n",
    "            central.append(dataset[0][d])\n",
    "            contx.append(dataset[1][d])\n",
    "            negs.append(dataset[2][d])\n",
    "              \n",
    "        pos_words.append(torch.from_numpy(np.asarray(central)))\n",
    "        pos_contexts.append(torch.from_numpy(np.asarray(contx)))\n",
    "        neg_contexts.extend(torch.from_numpy(np.asarray(negs)))\n",
    "    \n",
    "    return  pos_words, pos_contexts, neg_contexts\n",
    "\n",
    "pos_words, pos_contexts, neg_contexts = create_batches(dataset, batch_size)\n",
    "\n",
    "batched_dataset = {'pos_w': pos_words, 'pos_c': pos_contexts, 'neg_c':neg_contexts}\n",
    "\n",
    "with open('batched_dataset.p', 'wb') as f:\n",
    "    pickle.dump(batched_dataset, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('batched_dataset.p', 'rb') as f:\n",
    "    batched_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 38,   4,  34,  20,  47,  51,  37,  13,  11,  34])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_dataset['pos_w'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batched_dataset['pos_c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_batches = batched_dataset['pos_w']\n",
    "context_batches = batched_dataset['pos_c']\n",
    "neg_context_batches = batched_dataset['neg_c']\n",
    "\n",
    "no_batch = len(word_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocabulary_size, embedding_dim):\n",
    "        super(SkipGram, self).__init__()\n",
    "        \n",
    "        #sparse embeddings for word and context vectors\n",
    "        self.w_embeddings = nn.Embedding(vocabulary_size, embedding_dim) #, sparse = True\n",
    "        self.lin1 = nn.Linear(embedding_dim, vocabulary_size, bias = False)\n",
    "           \n",
    "    def forward(self, pos_words):\n",
    "        \n",
    "        out = self.w_embeddings(pos_words)\n",
    "        \n",
    "        out = self.lin1(out)\n",
    "        \n",
    "        final_out = F.log_softmax(out, dim = 0)\n",
    "        \n",
    "        return final_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248.24717736244202\n"
     ]
    }
   ],
   "source": [
    "model = SkipGram(vocab_size, embedding_dim)\n",
    "loss_func = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for e in range(epochs):\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    for b in range(no_batch):\n",
    "        \n",
    "        words = word_batches[b]\n",
    "        contexts = context_batches[b]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(words)\n",
    "        \n",
    "        loss = loss_func(preds, contexts)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    if (e+1)%10 == 0:\n",
    "        print(total_loss)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2402, -1.1282, -0.0895, -0.3965, -0.2605, -0.6403, -1.3293,\n",
       "        -0.7503,  0.5070, -0.2929, -1.0950,  0.2650, -1.1940,  0.2802,\n",
       "        -0.1430,  0.6097,  0.0903,  0.4231, -0.2599,  1.0438,  1.3403,\n",
       "        -3.3277,  0.6644,  0.9449,  0.2858, -0.2634,  0.4547, -0.4700,\n",
       "        -0.9312,  0.1417,  1.4448,  0.0696, -0.1068,  0.9950, -0.1015,\n",
       "        -0.3644, -0.2624, -0.5105, -0.2705, -0.7756, -0.6153,  0.4367,\n",
       "         0.2607, -0.3589,  0.7530,  0.5306,  0.0430, -0.5074, -0.0666,\n",
       "        -0.2776,  0.1563, -0.3429, -0.6137, -0.4683,  0.0128,  0.8209,\n",
       "        -0.6862, -0.7298, -0.5359, -0.0419,  0.9605, -1.0828,  0.7252,\n",
       "        -0.4318,  0.3126,  0.2615, -0.4629, -0.6999, -0.3502,  1.0422,\n",
       "        -0.4559,  0.5818,  0.9175, -0.0764, -0.0485, -0.2951, -1.7486,\n",
       "        -0.1142,  0.0655,  0.1596,  0.1953, -0.2336,  0.2528, -0.3813,\n",
       "         0.4473,  0.3742, -0.0122,  0.0698, -0.4119,  0.4317,  0.3617,\n",
       "        -1.0024,  0.4201, -0.2866,  0.7597,  0.7087,  0.8933,  0.1399,\n",
       "         0.3768,  0.2416])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.w_embeddings.weight[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramNeg(nn.Module):\n",
    "    def __init__(self, vocabulary_size, embedding_dim):\n",
    "        super(SkipGramNeg, self).__init__()\n",
    "        \n",
    "        #sparse embeddings for word and context vectors\n",
    "        \n",
    "        #initialization of embeds?\n",
    "        \n",
    "        self.w_embeddings = nn.Embedding(vocabulary_size, embedding_dim) #, sparse = True\n",
    "        self.lin1 = nn.Linear(embedding_dim, vocabulary_size, bias = False)\n",
    "        \n",
    "    def forward(self, pos_words):\n",
    "        \n",
    "        out = self.w_embeddings(pos_words)\n",
    "        \n",
    "        out = self.lin1(out)\n",
    "        \n",
    "        final_out = F.log_softmax(out, dim = 0)\n",
    "        \n",
    "        return final_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch, total loss, average loss, duration\n",
      "0 388.3636381626129 2.3116883224 0:00:00.056499\n",
      "1 298.03815948963165 2.04286249301 0:00:00.044088\n",
      "2 279.8476963043213 1.91716169436 0:00:00.044176\n",
      "3 269.78527426719666 1.83933745271 0:00:00.044625\n",
      "4 264.7503014802933 1.7866488925 0:00:00.054584\n",
      "5 258.9631435871124 1.74578195763 0:00:00.045191\n",
      "6 255.74946749210358 1.71385857209 0:00:00.042508\n",
      "7 252.07343870401382 1.68718089248 0:00:00.043858\n",
      "8 250.33574923872948 1.66528232059 0:00:00.043178\n",
      "9 246.82622891664505 1.64567446288 0:00:00.044878\n"
     ]
    }
   ],
   "source": [
    "epochs = 10 \n",
    "model = SkipGramNeg(vocab_size, embedding_dim)\n",
    "loss_func = nn.NLLLoss()\n",
    "#loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "losses = []\n",
    "avg_losses = []\n",
    "\n",
    "print('epoch, total loss, average loss, duration')\n",
    "for e in range(epochs):\n",
    "    \n",
    "    then = datetime.now()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    for b in range(no_batch):\n",
    "        \n",
    "        words = word_batches[b]\n",
    "        contexts = context_batches[b]\n",
    "        neg_contexts = neg_context_batches[b]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(words)\n",
    "        \n",
    "        loss = loss_func(preds, contexts)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    \n",
    "    now = datetime.now()\n",
    "        \n",
    "    losses.append(total_loss)\n",
    "    \n",
    "    avg_loss = np.mean(losses)/no_batch\n",
    "    \n",
    "    print(e, total_loss, avg_loss, now-then)\n",
    "    \n",
    "    avg_losses.append(avg_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n"
     ]
    }
   ],
   "source": [
    "print(len(preds[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(model):\n",
    "    \n",
    "    return model.w_embeddings.weight.data, model.lin1.weight.data\n",
    "\n",
    "def save_embeddings(embeds, file_name):\n",
    "    \n",
    "    with open(file_name, 'wb') as file:\n",
    "        pickle.dump(embeds.numpy(), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wm, cm = get_embeddings(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embeddings(wm, 'wordvecs_skipgram.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wordvecs_skipgram.pickle', 'rb') as file:\n",
    "    w_embeds = pickle.load(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
