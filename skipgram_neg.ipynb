{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import spatial\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "from random import randint\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('pos_data.p', 'rb') as f:\n",
    "    pos_data = pickle.load(f)\n",
    "    \n",
    "with open('neg_data.p', 'rb') as f:\n",
    "    neg_data = pickle.load(f)\n",
    "\n",
    "with open('unigram_probs.p', 'rb') as f:\n",
    "    unigram_probs = pickle.load(f)\n",
    "    \n",
    "vocab_size = len(unigram_probs)\n",
    "\n",
    "central_words = []\n",
    "contexts = []\n",
    "neg_samples = []\n",
    "\n",
    "for p in pos_data:\n",
    "    central_words.append(p[0])\n",
    "    contexts.append(p[1])\n",
    "    \n",
    "for n in neg_data:\n",
    "    neg_samples.append(n[1])\n",
    "    \n",
    "dataset = [central_words, contexts, neg_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_batches(dataset, batch_size):\n",
    "    \n",
    "    batch_number = len(dataset[0]) // batch_size\n",
    "    no_central_words = len(dataset[0])\n",
    "    pos_words = []\n",
    "    pos_contexts = []\n",
    "    neg_contexts = []\n",
    "    \n",
    "    for bn in range(batch_number):\n",
    "        indices = np.arange(0, no_central_words)\n",
    "        \n",
    "        #shuffle set\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        indices = indices[0:batch_size]\n",
    "        #shuffle dataset\n",
    "        \n",
    "        central = []\n",
    "        contx = []\n",
    "        negs = []\n",
    "        \n",
    "        for d in indices:\n",
    "            \n",
    "            central.append(dataset[0][d])\n",
    "            contx.append(dataset[1][d])\n",
    "            negs.append(dataset[2][d])\n",
    "              \n",
    "        pos_words.append(torch.from_numpy(np.asarray(central)))\n",
    "        pos_contexts.append(torch.from_numpy(np.asarray(contx)))\n",
    "        neg_contexts.append(torch.from_numpy(np.asarray(negs)))\n",
    "    \n",
    "    return  pos_words, pos_contexts, neg_contexts\n",
    "\n",
    "pos_words, pos_contexts, neg_contexts = create_batches(dataset, batch_size)\n",
    "\n",
    "batched_dataset = {'pos_w': pos_words, 'pos_c': pos_contexts, 'neg_c':neg_contexts}\n",
    "\n",
    "with open('batched_dataset.p', 'wb') as f:\n",
    "    pickle.dump(batched_dataset, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('batched_dataset.p', 'rb') as f:\n",
    "    batched_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  8,  67,  16,  56,  56,  71,  70,  42,  72,  78])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_dataset['pos_w'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batched_dataset['pos_c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_batches = batched_dataset['pos_w']\n",
    "context_batches = batched_dataset['pos_c']\n",
    "neg_context_batches = batched_dataset['neg_c']\n",
    "\n",
    "no_batch = len(word_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 64,  67,  45,  32,  54],\n",
       "        [ 73,  10,  34,  60,  10],\n",
       "        [ 55,  27,  70,   4,  50],\n",
       "        [ 78,  48,  64,  16,  75],\n",
       "        [ 44,  36,  71,  32,  22],\n",
       "        [ 29,  41,  48,  19,  28],\n",
       "        [ 77,  44,  11,  66,  49],\n",
       "        [  7,  66,   4,  73,  13],\n",
       "        [ 19,  15,  53,  33,  46],\n",
       "        [ 26,  76,  46,   5,  22]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_context_batches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocabulary_size, embedding_dim):\n",
    "        super(SkipGram, self).__init__()\n",
    "        \n",
    "        #sparse embeddings for word and context vectors\n",
    "        self.w_embeddings = nn.Embedding(vocabulary_size, embedding_dim) #, sparse = True\n",
    "        self.lin1 = nn.Linear(embedding_dim, vocabulary_size, bias = False)\n",
    "           \n",
    "    def forward(self, pos_words):\n",
    "        \n",
    "        out = self.w_embeddings(pos_words)\n",
    "        \n",
    "        out = self.lin1(out)\n",
    "        \n",
    "        final_out = F.log_softmax(out, dim = 0)\n",
    "        \n",
    "        return final_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250.34693259000778\n"
     ]
    }
   ],
   "source": [
    "model = SkipGram(vocab_size, embedding_dim)\n",
    "loss_func = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for e in range(epochs):\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    for b in range(no_batch):\n",
    "        \n",
    "        words = word_batches[b]\n",
    "        contexts = context_batches[b]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(words)\n",
    "        \n",
    "        loss = loss_func(preds, contexts)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    if (e+1)%10 == 0:\n",
    "        print(total_loss)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1757,  0.3037, -0.4713, -0.8476,  0.0191,  0.6583,  0.2654,\n",
       "         0.0535,  0.1119, -0.4707, -1.0232, -0.6034, -0.6984, -0.0050,\n",
       "        -0.8637, -0.8943,  0.5435,  0.4770,  0.6377, -1.2080,  1.5565,\n",
       "         0.2134, -0.4355,  0.9534,  0.2121, -0.2937, -0.2291,  0.2925,\n",
       "         0.6719,  0.3989,  0.9640, -1.1787,  0.4202, -0.8986, -1.8425,\n",
       "        -0.3739, -0.6442, -0.2802,  0.2347,  0.0256, -0.3941, -1.3290,\n",
       "        -1.0294,  0.1271,  1.5448, -0.6627, -0.2307,  0.7656,  0.8008,\n",
       "        -0.4602,  0.8287, -1.0531, -0.2534,  0.9345,  0.4222, -1.0691,\n",
       "         1.4121,  0.4038, -0.1973,  0.4030, -0.2278,  0.3137,  0.9210,\n",
       "         1.0097,  0.6797, -1.5831, -0.2292, -0.2369, -0.8806, -0.1497,\n",
       "         1.4229, -0.1659,  0.7073, -0.3759,  0.0603, -0.0784, -1.3290,\n",
       "        -0.0956, -0.7328,  0.3626, -0.3000, -1.6893, -0.9691,  0.2167,\n",
       "        -0.6114, -0.5323, -0.1796, -0.1267,  0.0232, -0.7965,  0.3290,\n",
       "         2.7254, -0.0419,  1.2038, -1.8167,  0.7708, -0.4069, -1.2394,\n",
       "         0.2046,  0.2218])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.w_embeddings.weight[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramNeg(nn.Module):\n",
    "    def __init__(self, vocabulary_size, embedding_dim):\n",
    "        super(SkipGramNeg, self).__init__()\n",
    "        \n",
    "        #sparse embeddings for word and context vectors\n",
    "        \n",
    "        self.w_embeddings = nn.Embedding(vocabulary_size, embedding_dim, sparse = True)\n",
    "        self.c_embeddings = nn.Embedding(vocabulary_size, embedding_dim, sparse = True)\n",
    "        \n",
    "#     initialization of embeds\n",
    "#     https://adoni.github.io/2017/11/08/word2vec-pytorch/\n",
    "\n",
    "#def init_emb(self):\n",
    "#     initrange = 0.5 / self.embedding_dim\n",
    "#     self.u_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "#     self.v_embeddings.weight.data.uniform_(-0, 0)\n",
    "\n",
    "    def forward(self, pos_words, pos_conts, neg_conts):\n",
    "        \n",
    "        #Loss calculation, Levy&Goldberg word2vec Explained\n",
    "        #https://adoni.github.io/2017/11/08/word2vec-pytorch/\n",
    "        \n",
    "        w_out = self.w_embeddings(pos_words)\n",
    "        \n",
    "        pos_out = self.c_embeddings(pos_conts)\n",
    "        neg_out = self.c_embeddings(neg_conts)\n",
    "        \n",
    "#         print(neg_conts)\n",
    "#         print(pos_conts)\n",
    "#         print(pos_words)\n",
    "               \n",
    "        pos_val = torch.mul(w_out, pos_out).squeeze()\n",
    "        pos_val = torch.sum(pos_val, dim = 1)\n",
    "        pos_loss = F.logsigmoid(pos_val)\n",
    "        \n",
    "        neg_val = torch.bmm(neg_out, w_out.unsqueeze(2)).squeeze()\n",
    "        neg_val = torch.sum(neg_val, dim = 1)\n",
    "        neg_loss = F.logsigmoid(-neg_val)\n",
    "        \n",
    "        final_out = pos_loss + neg_loss.sum()\n",
    "        final_out = -final_out.sum()/len(pos_words) #neg and mean\n",
    "         \n",
    "        return final_out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch, total loss, average loss, duration\n",
      "0 6825.353890698403 40.870382579 0:00:00.108423\n",
      "1 931.2951987367123 23.2235002678 0:00:00.102689\n",
      "2 420.3461255710572 16.3213477345 0:00:00.110398\n",
      "3 301.54100554808974 12.6924194918 0:00:00.103525\n",
      "4 226.53614019602537 10.4252363602 0:00:00.099595\n",
      "5 169.6015288978815 8.85695996971 0:00:00.094720\n",
      "6 128.27353544719517 7.70140926013 0:00:00.109288\n",
      "7 97.6550446646288 6.81182819593 0:00:00.101043\n",
      "8 75.03320912178606 6.10488069121 0:00:00.108342\n",
      "9 57.568934701383114 5.52886503807 0:00:00.170948\n",
      "10 44.56126679619774 5.05049857397 0:00:00.148143\n",
      "11 34.65767208207399 4.64691794035 0:00:00.164324\n",
      "12 26.931503540836275 4.30186782865 0:00:00.138341\n",
      "13 20.774830242153257 4.0034772824 0:00:00.110238\n",
      "14 15.90551572991535 3.74292830418 0:00:00.106839\n",
      "15 12.081460603745654 3.51351678989 0:00:00.099129\n",
      "16 9.166912177111953 3.31006825458 0:00:00.117032\n",
      "17 6.94044858077541 3.1284844389 0:00:00.163336\n",
      "18 5.265653137117624 2.96548688196 0:00:00.135329\n",
      "19 4.145218511577696 2.81845362125 0:00:00.098084\n",
      "20 3.444222295889631 2.68522364336 0:00:00.095111\n",
      "21 2.9323461265303195 2.5639661577 0:00:00.104459\n",
      "22 2.5000383511651307 2.45314025039 0:00:00.095056\n",
      "23 2.1181923884432763 2.35145456441 0:00:00.095155\n",
      "24 1.8131149557884783 2.25783066086 0:00:00.114165\n",
      "25 1.620500558288768 2.1713642353 0:00:00.103089\n",
      "26 1.4829800390871242 2.09127223103 0:00:00.106186\n",
      "27 1.3686721267877147 2.01687663854 0:00:00.102910\n",
      "28 1.2697838657768443 1.94759135777 0:00:00.094809\n",
      "29 1.1826568969991058 1.88290770511 0:00:00.112287\n",
      "30 1.1049891171278432 1.82238218886 0:00:00.094763\n",
      "31 1.0351462718099356 1.76562644797 0:00:00.094779\n",
      "32 0.9719088368583471 1.7122989742 0:00:00.106539\n",
      "33 0.9143213065108284 1.66209826843 0:00:00.100261\n",
      "34 0.8616291653597727 1.6147571595 0:00:00.123731\n",
      "35 0.813213511952199 1.57003805902 0:00:00.162565\n",
      "36 0.7685658088885248 1.52772898149 0:00:00.135446\n",
      "37 0.7272582004079595 1.48764018828 0:00:00.167049\n",
      "38 0.6889325706288218 1.44960134614 0:00:00.136288\n",
      "39 0.6532798658590764 1.41345910887 0:00:00.109847\n",
      "40 0.6200378600042313 1.37907505259 0:00:00.112803\n",
      "41 0.5889783560414799 1.34632390411 0:00:00.097028\n",
      "42 0.5598987461416982 1.31509201535 0:00:00.095113\n",
      "43 0.5326278143911622 1.28527604655 0:00:00.110646\n",
      "44 0.5070107135688886 1.25678182312 0:00:00.103285\n",
      "45 0.48291149333817884 1.22952334187 0:00:00.097147\n",
      "46 0.46020931500243023 1.20342190362 0:00:00.101467\n",
      "47 0.43879631999880075 1.17840535402 0:00:00.094807\n",
      "48 0.4185739867971279 1.15440741682 0:00:00.106162\n",
      "49 0.39945935900323093 1.13136710793 0:00:00.101034\n",
      "50 0.38137151551200077 1.10922821683 0:00:00.100995\n",
      "51 0.3642390872701071 1.08793884867 0:00:00.172600\n",
      "52 0.3479970768094063 1.06745101784 0:00:00.159473\n",
      "53 0.3325870303087868 1.04772028675 0:00:00.151800\n",
      "54 0.3179571581131313 1.02870544399 0:00:00.104296\n",
      "55 0.3040569297154434 1.01036821643 0:00:00.094868\n",
      "56 0.2908412562974263 0.992673012004 0:00:00.095505\n",
      "57 0.2782694855995942 0.975586689113 0:00:00.112621\n",
      "58 0.26630412944359705 0.959078349221 0:00:00.113193\n",
      "59 0.2549083296034951 0.943119150021 0:00:00.102988\n",
      "60 0.2440507260034792 0.927682137424 0:00:00.109999\n",
      "61 0.2336997234087903 0.912742093264 0:00:00.121385\n",
      "62 0.22382939982344396 0.898275398066 0:00:00.118971\n",
      "63 0.2144130077213049 0.884259906069 0:00:00.160782\n",
      "64 0.20542573611601256 0.870674832041 0:00:00.158210\n",
      "65 0.19684639628394507 0.857500648539 0:00:00.168035\n",
      "66 0.18865167527110316 0.844718991856 0:00:00.165734\n",
      "67 0.18082223559031263 0.832312576797 0:00:00.168530\n",
      "68 0.17334031392238103 0.820265118669 0:00:00.110667\n",
      "69 0.16618934908183292 0.808561261914 0:00:00.110839\n",
      "70 0.15935185804846697 0.797186514601 0:00:00.113571\n",
      "71 0.1528131225204561 0.786127188685 0:00:00.111768\n",
      "72 0.14655546614085324 0.775370344698 0:00:00.113552\n",
      "73 0.14057107604457997 0.764903741972 0:00:00.113723\n",
      "74 0.13484332768712193 0.754715791347 0:00:00.111604\n",
      "75 0.12935970464604907 0.744795512632 0:00:00.112258\n",
      "76 0.12411126794177108 0.735132495341 0:00:00.108381\n",
      "77 0.11908504542952869 0.725716861864 0:00:00.110392\n",
      "78 0.11427251432905905 0.716539234075 0:00:00.112168\n",
      "79 0.10966101060330402 0.707590701809 0:00:00.112282\n",
      "80 0.10524414975952823 0.698862794434 0:00:00.111646\n",
      "81 0.10101307059812825 0.690347453876 0:00:00.112149\n",
      "82 0.09695886500412598 0.682037009758 0:00:00.102679\n",
      "83 0.0930736316513503 0.673924156393 0:00:00.118178\n",
      "84 0.08934909572417382 0.666001931312 0:00:00.162869\n",
      "85 0.08577925838471856 0.658263695463 0:00:00.129567\n",
      "86 0.08235593495192006 0.650703114472 0:00:00.149119\n",
      "87 0.07907521807646845 0.64331414163 0:00:00.121554\n",
      "88 0.07593051630828995 0.636091001541 0:00:00.168068\n",
      "89 0.07291194533900125 0.62902817484 0:00:00.116571\n",
      "90 0.07001860838499852 0.622120384711 0:00:00.109598\n",
      "91 0.06724402724648826 0.615362583343 0:00:00.116533\n",
      "92 0.06458418932015775 0.608749939777 0:00:00.097865\n",
      "93 0.062031230198044796 0.602277828125 0:00:00.100610\n",
      "94 0.0595798125868896 0.595941816938 0:00:00.114170\n",
      "95 0.05723104667413281 0.589737659479 0:00:00.109058\n",
      "96 0.05497706392634427 0.583661283649 0:00:00.096341\n",
      "97 0.052813596557825804 0.577708783296 0:00:00.113399\n",
      "98 0.05073736445046961 0.571876409895 0:00:00.120489\n",
      "99 0.04874550366366748 0.566160564688 0:00:00.106745\n",
      "100 0.04683361345814774 0.560557791184 0:00:00.110488\n",
      "101 0.04499939919332974 0.555064768305 0:00:00.115922\n",
      "102 0.04323915000713896 0.54967830373 0:00:00.114870\n",
      "103 0.04154818724055076 0.544395327651 0:00:00.095317\n",
      "104 0.039923822914715856 0.539212887053 0:00:00.099680\n",
      "105 0.03836891198079684 0.534128140514 0:00:00.129835\n",
      "106 0.036871837171929656 0.529138352186 0:00:00.108334\n",
      "107 0.035435547230008524 0.524240887711 0:00:00.116624\n",
      "108 0.03405866374305333 0.51943320933 0:00:00.111149\n",
      "109 0.03273428925967892 0.514712871188 0:00:00.104789\n",
      "110 0.03146272467711242 0.510077515587 0:00:00.110798\n",
      "111 0.030243246888858266 0.505524868995 0:00:00.099320\n",
      "112 0.029070780710753752 0.501052738088 0:00:00.106867\n",
      "113 0.027945424440986244 0.496659006508 0:00:00.107057\n",
      "114 0.026865440373512683 0.492341631415 0:00:00.108618\n",
      "115 0.025829293193964986 0.488098640338 0:00:00.103496\n",
      "116 0.02483175217275857 0.483928127969 0:00:00.111212\n",
      "117 0.023875489410784212 0.479828253723 0:00:00.102753\n",
      "118 0.022957646102440776 0.475797238742 0:00:00.132384\n",
      "119 0.022074349675676785 0.471833363267 0:00:00.146280\n",
      "120 0.021225840555416653 0.467934964404 0:00:00.112552\n",
      "121 0.020411988647538237 0.464100433773 0:00:00.112697\n",
      "122 0.019631282091722824 0.460328215227 0:00:00.108200\n",
      "123 0.018879477898735786 0.45661680261 0:00:00.146137\n",
      "124 0.018159224006012664 0.452964738092 0:00:00.146782\n",
      "125 0.01746582465057145 0.449370609899 0:00:00.119189\n",
      "126 0.016801259933345136 0.445833050816 0:00:00.095245\n",
      "127 0.016161620607817895 0.442350736171 0:00:00.099204\n",
      "128 0.015547181941656163 0.438922382378 0:00:00.136343\n",
      "129 0.01495726498978911 0.435546745317 0:00:00.149547\n",
      "130 0.014390905256732367 0.432222618811 0:00:00.115573\n",
      "131 0.013845695444615558 0.428948833126 0:00:00.158102\n",
      "132 0.013323638624569867 0.425724253796 0:00:00.121768\n",
      "133 0.01282024182000896 0.422547780021 0:00:00.109490\n",
      "134 0.012337972627392446 0.419418343725 0:00:00.114267\n",
      "135 0.011873875528181088 0.416334908117 0:00:00.107162\n",
      "136 0.01142836831331806 0.413296466695 0:00:00.126343\n",
      "137 0.010999150513271161 0.410302042032 0:00:00.149114\n",
      "138 0.010587474405838293 0.407350684881 0:00:00.124176\n",
      "139 0.010192994674980582 0.404441473103 0:00:00.136966\n",
      "140 0.009811336643906543 0.401573510531 0:00:00.112491\n",
      "141 0.009446279621442955 0.398745926405 0:00:00.111535\n",
      "142 0.009094605483369378 0.395957874183 0:00:00.116227\n",
      "143 0.008756898293540871 0.393208530866 0:00:00.111423\n",
      "144 0.008432574451489927 0.390497096132 0:00:00.111296\n",
      "145 0.008120024889649358 0.387822791521 0:00:00.111774\n",
      "146 0.007819798252512555 0.385184859777 0:00:00.119178\n",
      "147 0.007532550145697314 0.382582564137 0:00:00.114032\n",
      "148 0.007254752363223815 0.380015187476 0:00:00.109760\n",
      "149 0.0069854753573963535 0.377482031754 0:00:00.116036\n"
     ]
    }
   ],
   "source": [
    "epochs = 150\n",
    "model = SkipGramNeg(vocab_size, embedding_dim)\n",
    "optimizer = optim.SparseAdam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "losses = []\n",
    "avg_losses = []\n",
    "\n",
    "print('epoch, total loss, average loss, duration')\n",
    "for e in range(epochs):\n",
    "    \n",
    "    then = datetime.now()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    for b in range(no_batch):\n",
    "        \n",
    "        words = word_batches[b]\n",
    "        contexts = context_batches[b]\n",
    "        neg_contexts = neg_context_batches[b]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = model(words, contexts, neg_contexts)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()       \n",
    "    \n",
    "    now = datetime.now()\n",
    "        \n",
    "    losses.append(total_loss)\n",
    "    \n",
    "    avg_loss = np.mean(losses)/no_batch\n",
    "    \n",
    "    print(e, total_loss, avg_loss, now-then)\n",
    "    \n",
    "    avg_losses.append(avg_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embeddings(model):\n",
    "    \n",
    "    return model.w_embeddings.weight.data, model.c_embeddings.weight.data\n",
    "\n",
    "def save_embeddings(embeds, file_name):\n",
    "    \n",
    "    with open(file_name, 'wb') as file:\n",
    "        pickle.dump(embeds.numpy(), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm, cm = get_embeddings(model)\n",
    "\n",
    "save_embeddings(wm, 'wordvecs_skipgram_word.pickle')\n",
    "\n",
    "save_embeddings(cm, 'wordvecs_skipgram_context.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('wordvecs_skipgram.pickle', 'rb') as file:\n",
    "    w_embeds = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79, 100)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visualize tsne for embeds\n",
    "#each epoch save embedding weight as pickles!!!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
